{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bottom-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from typing import *\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pd.set_option('max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "naval-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "import yaml\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # General\n",
    "    device: str = 'cuda'\n",
    "    # Data config\n",
    "    imgconf_file: str = '../../data/VinBigData/train.csv'\n",
    "    imgdir_name: str = \"../../data/VinBigData/png256\"\n",
    "    # Model\n",
    "    model_dir: str = \"../ChestXRay14-2Class/results\"\n",
    "    model_mode: str = 'normal'\n",
    "    model_name: str = 'resnet18'\n",
    "    \n",
    "    # Evaluation\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 4\n",
    "\n",
    "    def update(self, param_dict: Dict) -> \"Config\":\n",
    "        # Overwrite by `param_dict`\n",
    "        for key, value in param_dict.items():\n",
    "            if not hasattr(self, key):\n",
    "                raise ValueError(f\"[ERROR] Unexpected key for flag = {key}\")\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "    \n",
    "    def to_yaml(self, filepath: str, width: int = 120):\n",
    "        with open(filepath, 'w') as f:\n",
    "            yaml.dump(asdict(self), f, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "toxic-indie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu101 True\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "import torch\n",
    "assert torch.__version__.startswith(\"1.7\")\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "\n",
    "def get_vinbigdata_dicts(\n",
    "    imgdir: Path,\n",
    "    train_df: pd.DataFrame,\n",
    "    train_data_type: str = \"original\",\n",
    "    use_cache: bool = True,\n",
    "    debug: bool = True,\n",
    "    target_indices: Optional[np.ndarray] = None,\n",
    "):\n",
    "    debug_str = f\"_debug{int(debug)}\"\n",
    "    train_data_type_str = f\"_{train_data_type}\"\n",
    "    cache_path = Path(\".\") / f\"dataset_dicts_cache{train_data_type_str}{debug_str}.pkl\"\n",
    "    if not use_cache or not cache_path.exists():\n",
    "        print(\"Creating data...\")\n",
    "        train_meta = pd.read_csv(imgdir / \"train_meta.csv\")\n",
    "        if debug:\n",
    "            train_meta = train_meta.iloc[:500]  # For debug....\n",
    "\n",
    "        # Load 1 image to get image size.\n",
    "        image_id = train_meta.loc[0, \"image_id\"]\n",
    "        image_path = str(imgdir / \"train\" / f\"{image_id}.png\")\n",
    "        image = cv2.imread(image_path)\n",
    "        resized_height, resized_width, ch = image.shape\n",
    "        print(f\"image shape: {image.shape}\")\n",
    "\n",
    "        dataset_dicts = []\n",
    "        for index, train_meta_row in tqdm(train_meta.iterrows(), total=len(train_meta)):\n",
    "            record = {}\n",
    "\n",
    "            image_id, height, width = train_meta_row.values\n",
    "            filename = str(imgdir / \"train\" / f\"{image_id}.png\")\n",
    "            record[\"file_name\"] = filename\n",
    "            record[\"image_id\"] = image_id\n",
    "            record[\"height\"] = resized_height\n",
    "            record[\"width\"] = resized_width\n",
    "            objs = []\n",
    "            for index2, row in train_df.query(\"image_id == @image_id\").iterrows():\n",
    "                # print(row)\n",
    "                # print(row[\"class_name\"])\n",
    "                # class_name = row[\"class_name\"]\n",
    "                class_id = row[\"class_id\"]\n",
    "                if class_id == 14:\n",
    "                    # It is \"No finding\"\n",
    "                    # This annotator does not find anything, skip.\n",
    "                    pass\n",
    "                else:\n",
    "                    # bbox_original = [int(row[\"x_min\"]), int(row[\"y_min\"]), int(row[\"x_max\"]), int(row[\"y_max\"])]\n",
    "                    h_ratio = resized_height / height\n",
    "                    w_ratio = resized_width / width\n",
    "                    bbox_resized = [\n",
    "                        int(row[\"x_min\"]) * w_ratio,\n",
    "                        int(row[\"y_min\"]) * h_ratio,\n",
    "                        int(row[\"x_max\"]) * w_ratio,\n",
    "                        int(row[\"y_max\"]) * h_ratio,\n",
    "                    ]\n",
    "                    obj = {\n",
    "                        \"bbox\": bbox_resized,\n",
    "                        \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                        \"category_id\": class_id,\n",
    "                    }\n",
    "                    objs.append(obj)\n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "        with open(cache_path, mode=\"wb\") as f:\n",
    "            pickle.dump(dataset_dicts, f)\n",
    "\n",
    "    print(f\"Load from cache {cache_path}\")\n",
    "    with open(cache_path, mode=\"rb\") as f:\n",
    "        dataset_dicts = pickle.load(f)\n",
    "    if target_indices is not None:\n",
    "        dataset_dicts = [dataset_dicts[i] for i in target_indices]\n",
    "    return dataset_dicts\n",
    "\n",
    "\n",
    "def get_vinbigdata_dicts_test(\n",
    "    imgdir: Path, test_meta: pd.DataFrame, use_cache: bool = True, debug: bool = True,\n",
    "):\n",
    "    debug_str = f\"_debug{int(debug)}\"\n",
    "    cache_path = Path(\".\") / f\"dataset_dicts_cache_test{debug_str}.pkl\"\n",
    "    if not use_cache or not cache_path.exists():\n",
    "        print(\"Creating data...\")\n",
    "        # test_meta = pd.read_csv(imgdir / \"test_meta.csv\")\n",
    "        if debug:\n",
    "            test_meta = test_meta.iloc[:500]  # For debug....\n",
    "\n",
    "        # Load 1 image to get image size.\n",
    "        image_id = test_meta.loc[0, \"image_id\"]\n",
    "        image_path = str(imgdir / \"test\" / f\"{image_id}.png\")\n",
    "        image = cv2.imread(image_path)\n",
    "        resized_height, resized_width, ch = image.shape\n",
    "        print(f\"image shape: {image.shape}\")\n",
    "\n",
    "        dataset_dicts = []\n",
    "        for index, test_meta_row in tqdm(test_meta.iterrows(), total=len(test_meta)):\n",
    "            record = {}\n",
    "\n",
    "            image_id, height, width = test_meta_row.values\n",
    "            filename = str(imgdir / \"test\" / f\"{image_id}.png\")\n",
    "            record[\"file_name\"] = filename\n",
    "            # record[\"image_id\"] = index\n",
    "            record[\"image_id\"] = image_id\n",
    "            record[\"height\"] = resized_height\n",
    "            record[\"width\"] = resized_width\n",
    "            # objs = []\n",
    "            # record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "        with open(cache_path, mode=\"wb\") as f:\n",
    "            pickle.dump(dataset_dicts, f)\n",
    "\n",
    "    print(f\"Load from cache {cache_path}\")\n",
    "    with open(cache_path, mode=\"rb\") as f:\n",
    "        dataset_dicts = pickle.load(f)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "piano-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fail to import apex_C: apex was not installed or installed without --cpp_ext.\n",
      "fail to import amp_C: apex was not installed or installed without --cpp_ext.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "\n",
    "class CNNFixedPredictor(nn.Module):\n",
    "    def __init__(self, cnn: nn.Module, num_classes: int = 2):\n",
    "        super(CNNFixedPredictor, self).__init__()\n",
    "        self.cnn = cnn\n",
    "        self.lin = Linear(cnn.num_features, num_classes)\n",
    "        print(\"cnn.num_features\", cnn.num_features)\n",
    "\n",
    "        # We do not learn CNN parameters.\n",
    "        # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.cnn(x)\n",
    "        return self.lin(feat)\n",
    "\n",
    "\n",
    "def build_predictor(model_name: str, model_mode: str = \"normal\"):\n",
    "    if model_mode == \"normal\":\n",
    "        # normal configuration. train all parameters.\n",
    "        return timm.create_model(model_name, pretrained=True, num_classes=2, in_chans=3)\n",
    "    elif model_mode == \"cnn_fixed\":\n",
    "        # normal configuration. train all parameters.\n",
    "        # https://rwightman.github.io/pytorch-image-models/feature_extraction/\n",
    "        timm_model = timm.create_model(model_name, pretrained=True, num_classes=0, in_chans=3)\n",
    "        return CNNFixedPredictor(timm_model, num_classes=2)\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unexpected value model_mode={model_mode}\")\n",
    "\n",
    "        \n",
    "def accuracy(y: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Computes multi-class classification accuracy\"\"\"\n",
    "    assert y.shape[:-1] == t.shape, f\"y {y.shape}, t {t.shape} is inconsistent.\"\n",
    "    pred_label = torch.max(y.detach(), dim=-1)[1]\n",
    "    count = t.nelement()\n",
    "    correct = (pred_label == t).sum().float()\n",
    "    acc = correct / count\n",
    "    return acc\n",
    "\n",
    "\n",
    "def accuracy_with_logits(y: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Computes multi-class classification accuracy\"\"\"\n",
    "    assert y.shape == t.shape\n",
    "    gt_label = torch.max(t.detach(), dim=-1)[1]\n",
    "    return accuracy(y, gt_label)\n",
    "\n",
    "\n",
    "def cross_entropy_with_logits(input, target, dim=-1):\n",
    "    loss = torch.sum(- target * F.log_softmax(input, dim), dim)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import pytorch_pfn_extras as ppe\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"two class classfication\"\"\"\n",
    "\n",
    "    def __init__(self, predictor, lossfun=cross_entropy_with_logits):\n",
    "        super().__init__()\n",
    "        self.predictor = predictor\n",
    "        self.lossfun = lossfun\n",
    "        self.prefix = \"\"\n",
    "\n",
    "    def forward(self, image, targets):\n",
    "        outputs = self.predictor(image)\n",
    "        loss = self.lossfun(outputs, targets)\n",
    "        metrics = {\n",
    "            f\"{self.prefix}loss\": loss.item(),\n",
    "            f\"{self.prefix}acc\": accuracy_with_logits(outputs, targets).item()\n",
    "        }\n",
    "        ppe.reporting.report(metrics, self)\n",
    "        return loss, metrics\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        pred = self.predict_proba(data_loader)\n",
    "        label = torch.argmax(pred, dim=1)\n",
    "        return label\n",
    "\n",
    "    def predict_proba(self, data_loader):\n",
    "        device: torch.device = next(self.parameters()).device\n",
    "        y_list = []\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                if isinstance(batch, (tuple, list)):\n",
    "                    # Assumes first argument is \"image\"\n",
    "                    batch = batch[0].to(device)\n",
    "                else:\n",
    "                    batch = batch.to(device)\n",
    "                y = self.predictor(batch)\n",
    "                y = torch.softmax(y, dim=-1)\n",
    "                y_list.append(y)\n",
    "        pred = torch.cat(y_list)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mysterious-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Referenced `chainer.dataset.DatasetMixin` to work with pytorch Dataset.\n",
    "\"\"\"\n",
    "import numpy\n",
    "import six\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class DatasetMixin(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns an example or a sequence of examples.\"\"\"\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        if isinstance(index, slice):\n",
    "            current, stop, step = index.indices(len(self))\n",
    "            return [self.get_example_wrapper(i) for i in\n",
    "                    six.moves.range(current, stop, step)]\n",
    "        elif isinstance(index, list) or isinstance(index, numpy.ndarray):\n",
    "            return [self.get_example_wrapper(i) for i in index]\n",
    "        else:\n",
    "            return self.get_example_wrapper(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of data points.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_example_wrapper(self, i):\n",
    "        \"\"\"Wrapper of `get_example`, to apply `transform` if necessary\"\"\"\n",
    "        example = self.get_example(i)\n",
    "        if self.transform:\n",
    "            example = self.transform(example)\n",
    "        return example\n",
    "\n",
    "    def get_example(self, i):\n",
    "        \"\"\"Returns the i-th example.\n",
    "\n",
    "        Implementations should override it. It should raise :class:`IndexError`\n",
    "        if the index is invalid.\n",
    "\n",
    "        Args:\n",
    "            i (int): The index of the example.\n",
    "\n",
    "        Returns:\n",
    "            The i-th example.\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VinbigdataTwoClassDataset(DatasetMixin):\n",
    "    def __init__(self, dataset_dicts, image_transform=None, transform=None, train: bool = True,\n",
    "                 mixup_prob: float = -1.0, label_smoothing: float = 0.0):\n",
    "        super(VinbigdataTwoClassDataset, self).__init__(transform=transform)\n",
    "        self.dataset_dicts = dataset_dicts\n",
    "        self.image_transform = image_transform\n",
    "        self.train = train\n",
    "        self.mixup_prob = mixup_prob\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def _get_single_example(self, i):\n",
    "        d = self.dataset_dicts[i]\n",
    "        filename = d[\"file_name\"]\n",
    "\n",
    "        img = cv2.imread(filename)\n",
    "        if self.image_transform:\n",
    "            img = self.image_transform(img)\n",
    "        img = torch.tensor(np.transpose(img, (2, 0, 1)).astype(np.float32))\n",
    "\n",
    "        if self.train:\n",
    "            label = int(len(d[\"annotations\"]) > 0)  # 0 normal, 1 abnormal\n",
    "            if self.label_smoothing > 0:\n",
    "                if label == 0:\n",
    "                    return img, float(label) + self.label_smoothing\n",
    "                else:\n",
    "                    return img, float(label) - self.label_smoothing\n",
    "            else:\n",
    "                return img, float(label)\n",
    "        else:\n",
    "            # Only return img\n",
    "            return img, None\n",
    "\n",
    "    def get_example(self, i):\n",
    "        img, label = self._get_single_example(i)\n",
    "        if self.mixup_prob > 0. and np.random.uniform() < self.mixup_prob:\n",
    "            j = np.random.randint(0, len(self.dataset_dicts))\n",
    "            p = np.random.uniform()\n",
    "            img2, label2 = self._get_single_example(j)\n",
    "            img = img * p + img2 * (1 - p)\n",
    "            if self.train:\n",
    "                label = label * p + label2 * (1 - p)\n",
    "\n",
    "        if self.train:\n",
    "            label_logit = torch.tensor([1 - label, label], dtype=torch.float32)\n",
    "            return img, label_logit\n",
    "        else:\n",
    "            # Only return img\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "following-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {}\n",
    "config = Config().update(config_dict)\n",
    "base_dir = Path().resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "complimentary-professional",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(device='cuda', imgconf_file='../../data/VinBigData/train.csv', imgdir_name='../../data/VinBigData/png256', model_dir='../ChestXRay14-2Class/results', model_mode='normal', model_name='resnet18', batch_size=32, num_workers=4)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "meaning-attendance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(config.device)\n",
    "predictor = build_predictor(model_name=config.model_name, model_mode=config.model_mode)\n",
    "predictor.load_state_dict(torch.load(base_dir / config.model_dir / 'predictor_last.pt'))\n",
    "predictor.eval()\n",
    "\n",
    "classifier = Classifier(predictor)\n",
    "classifier.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ignored-particular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "image shape: (256, 256, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6998c9d06e42e584c8834e261bec23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from cache dataset_dicts_cache_original_debug0.pkl\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(base_dir / config.imgconf_file)\n",
    "\n",
    "dataset_dicts = get_vinbigdata_dicts(base_dir / config.imgdir_name, train, debug=False)\n",
    "dataset = VinbigdataTwoClassDataset(dataset_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "hispanic-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "\n",
    "valid_dataset = VinbigdataTwoClassDataset(dataset_dicts)\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "valid_pred = classifier.predict_proba(valid_loader).cpu().numpy()\n",
    "valid_pred_df = pd.DataFrame({\n",
    "    \"image_id\": [dd[\"image_id\"] for dd in dataset_dicts],\n",
    "    \"class0\": valid_pred[:, 0],\n",
    "    \"class1\": valid_pred[:, 1],\n",
    "    \"truth\": [int(len(d[\"annotations\"]) > 0) for d in dataset_dicts],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "separated-projector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class0</th>\n",
       "      <th>class1</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4d390e07733ba06e5ff07412f09c0a92</td>\n",
       "      <td>0.863667</td>\n",
       "      <td>1.363332e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>289f69f6462af4933308c275d07060f0</td>\n",
       "      <td>0.999127</td>\n",
       "      <td>8.730373e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68335ee73e67706aa59b8b55b54b11a4</td>\n",
       "      <td>0.760983</td>\n",
       "      <td>2.390170e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7ecd6f67f649f26c05805c8359f9e528</td>\n",
       "      <td>0.996463</td>\n",
       "      <td>3.536960e-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2229148faa205e881cf0d932755c9e40</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>3.084141e-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>b3510302f95f75a91e0fd49e04767f02</td>\n",
       "      <td>0.999721</td>\n",
       "      <td>2.785048e-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>dcb081bb5e1dac41000e96fc37c8c322</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>5.494593e-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>c619a784636c085eb798f98a5ba1102d</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.472597e-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>e7ea94b2fec95a7461b10a7f3eea2897</td>\n",
       "      <td>0.740925</td>\n",
       "      <td>2.590748e-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>56d8197180bcebfbf00bb64cc694ea8c</td>\n",
       "      <td>0.917511</td>\n",
       "      <td>8.248943e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               image_id    class0        class1  truth\n",
       "0      4d390e07733ba06e5ff07412f09c0a92  0.863667  1.363332e-01      0\n",
       "1      289f69f6462af4933308c275d07060f0  0.999127  8.730373e-04      0\n",
       "2      68335ee73e67706aa59b8b55b54b11a4  0.760983  2.390170e-01      0\n",
       "3      7ecd6f67f649f26c05805c8359f9e528  0.996463  3.536960e-03      1\n",
       "4      2229148faa205e881cf0d932755c9e40  0.999969  3.084141e-05      1\n",
       "...                                 ...       ...           ...    ...\n",
       "14995  b3510302f95f75a91e0fd49e04767f02  0.999721  2.785048e-04      1\n",
       "14996  dcb081bb5e1dac41000e96fc37c8c322  0.999945  5.494593e-05      1\n",
       "14997  c619a784636c085eb798f98a5ba1102d  1.000000  3.472597e-09      1\n",
       "14998  e7ea94b2fec95a7461b10a7f3eea2897  0.740925  2.590748e-01      1\n",
       "14999  56d8197180bcebfbf00bb64cc694ea8c  0.917511  8.248943e-02      0\n",
       "\n",
       "[15000 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "finite-shelf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamaguchi-milkcocholate/.pyenv/versions/3.7.6/envs/VinBigData/lib/python3.7/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa5ffe46950>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(valid_pred_df[\"class0\"].values, color='green', label='valid pred')\n",
    "plt.title(\"Prediction results histogram\")\n",
    "plt.xlim([0., 1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "recovered-preparation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6986666666666667"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "accuracy_score(valid_pred_df['class1'].values > 0.5, valid_pred_df['truth'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-mauritius",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prediction ---\n",
    "# valid data\n",
    "\n",
    "\n",
    "# test data\n",
    "test_meta = pd.read_csv(inputdir / \"vinbigdata-testmeta\" / \"test_meta.csv\")\n",
    "dataset_dicts_test = get_vinbigdata_dicts_test(imgdir, test_meta, debug=debug)\n",
    "test_dataset = VinbigdataTwoClassDataset(dataset_dicts_test, train=False)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=flags.valid_batchsize,\n",
    "    num_workers=flags.num_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_pred = classifier.predict_proba(test_loader).cpu().numpy()\n",
    "test_pred_df = pd.DataFrame({\n",
    "    \"image_id\": [d[\"image_id\"] for d in dataset_dicts_test],\n",
    "    \"class0\": test_pred[:, 0],\n",
    "    \"class1\": test_pred[:, 1]\n",
    "})\n",
    "test_pred_df.to_csv(outdir/\"test_pred.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
