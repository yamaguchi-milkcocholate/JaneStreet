{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "guilty-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from typing import *\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pd.set_option('max_columns', 50)\n",
    "pd.set_option('max_rows', 200)\n",
    "warnings.simplefilter('ignore')\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prerequisite-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fail to import apex_C: apex was not installed or installed without --cpp_ext.\n",
      "fail to import amp_C: apex was not installed or installed without --cpp_ext.\n",
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n",
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HACKING: overriding COCOeval.summarize = vin_summarize...\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path().resolve()\n",
    "sys.path.append(str(base_dir / '../'))\n",
    "\n",
    "from utils.preprocess import *\n",
    "from utils.model import *\n",
    "from utils.train import *\n",
    "from utils.eval import *\n",
    "from utils.detectron2helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "local-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int, device: str):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    with torch.cuda.device(device):\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "whole-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "import yaml\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # General\n",
    "    debug: bool = False\n",
    "    outdir: str = \"detectron2_results/[tmp]\"\n",
    "    device: str = \"cuda:0\"\n",
    "    device_id: int = 0\n",
    "    num_workers: int = 8\n",
    "\n",
    "    # Data config\n",
    "    imgconf_file: str = '../../data/VinBigData/train.csv'\n",
    "    meta_file: str = '../../data/VinBigData/train_meta.csv'\n",
    "    test_meta_file: str = '../../data/VinBigData/test_meta.csv'\n",
    "    imgdir_name: str = \"../../data/VinBigData/png[tmp]\"\n",
    "    seed: int = 111\n",
    "    n_splits: int = 5\n",
    "    iou_thr: float = 0.4\n",
    "    skip_box_thr: float = 0.1\n",
    "    sigma: float = 0.1\n",
    "    \n",
    "#     model_config: str = 'COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml'\n",
    "#     model_config: str = 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml'\n",
    "#     model_config: str = 'Misc/cascade_mask_rcnn_R_50_FPN_3x.yaml'\n",
    "#     model_config: str = 'COCO-Detection/retinanet_R_101_FPN_3x.yaml'\n",
    "#     model_config: str = 'COCO-Detection/retinanet_R_50_FPN_3x.yaml'\n",
    "    \n",
    "    # Training config\n",
    "    batch_size: int = 8\n",
    "    iter: int = 10000\n",
    "    lr_scheduler_name: str = \"WarmupCosineLR\"  # WarmupMultiStepLR (default) or WarmupCosineLR\n",
    "    base_lr: float = 0.00025\n",
    "    roi_batch_size_per_image: int = 512\n",
    "    eval_period: int = 10000\n",
    "    checkpoint_period: int = 10000\n",
    "        \n",
    "    aug_kwargs: Dict = field(default_factory=lambda: {})\n",
    "        \n",
    "\n",
    "    def update(self, param_dict: Dict) -> \"Config\":\n",
    "        # Overwrite by `param_dict`\n",
    "        for key, value in param_dict.items():\n",
    "            if not hasattr(self, key):\n",
    "                raise ValueError(f\"[ERROR] Unexpected key for flag = {key}\")\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "    \n",
    "    def to_yaml(self, filepath: str, width: int = 120):\n",
    "        with open(filepath, 'w') as f:\n",
    "            yaml.dump(asdict(self), f, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distinguished-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'debug': False,\n",
    "    'batch_size': 25,\n",
    "    'base_lr': 1e-3,\n",
    "    'iter': 10000,\n",
    "    'eval_period': 1000,\n",
    "    'checkpoint_period': 1000,\n",
    "    \"aug_kwargs\": {\n",
    "        \"HorizontalFlip\": {\"p\": 0.5},\n",
    "        \"ShiftScaleRotate\": {\"scale_limit\": 0.15, \"rotate_limit\": 10, \"p\": 0.5},\n",
    "        \"RandomBrightnessContrast\": {\"p\": 0.5},\n",
    "    },\n",
    "    'iou_thr': 0.4,\n",
    "    'skip_box_thr': 0.05,\n",
    "}\n",
    "config = Config().update(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "trained-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_nms = [\n",
    "    \"Aortic enlargement\",\n",
    "    \"Atelectasis\",\n",
    "    \"Calcification\",\n",
    "    \"Cardiomegaly\",\n",
    "    \"Consolidation\",\n",
    "    \"ILD\",\n",
    "    \"Infiltration\",\n",
    "    \"Lung Opacity\",\n",
    "    \"Nodule/Mass\",\n",
    "    \"Other lesion\",\n",
    "    \"Pleural effusion\",\n",
    "    \"Pleural thickening\",\n",
    "    \"Pneumothorax\",\n",
    "    \"Pulmonary fibrosis\",\n",
    "#     \"No Finding\"\n",
    "]\n",
    "classes_dict = {index: class_name  for index, class_name in enumerate(classes_nms)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continued-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta = pd.read_csv(str(base_dir / config.test_meta_file))\n",
    "# dataset_dicts_test = get_vinbigdata_dicts_test(\n",
    "#     base_dir / config.imgdir_name, \n",
    "#     test_meta,\n",
    "#     test_data_type=f'png{config.img_size}',\n",
    "#     debug=config.debug\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "closed-raise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from cache dataset_dicts_cache_test_png512_debug0.pkl\n",
      "Load from cache dataset_dicts_cache_test_png1024_debug0.pkl\n"
     ]
    }
   ],
   "source": [
    "img_size = 512\n",
    "imgdir_name = config.imgdir_name.replace('[tmp]', str(img_size))\n",
    "\n",
    "DatasetCatalog.register(\n",
    "    f\"vinbigdata_test_{img_size}\", lambda: get_vinbigdata_dicts_test(imgdir_name, test_meta, debug=config.debug)\n",
    ")\n",
    "MetadataCatalog.get(f\"vinbigdata_test_{img_size}\").set(thing_classes=classes_nms)\n",
    "metadata_512= MetadataCatalog.get(f\"vinbigdata_test_{img_size}\")\n",
    "dataset_dicts_512 = get_vinbigdata_dicts_test(\n",
    "    base_dir / imgdir_name, \n",
    "    test_meta,\n",
    "    test_data_type=f'png{img_size}',\n",
    "    debug=config.debug\n",
    ")\n",
    "\n",
    "img_size = 1024\n",
    "imgdir_name = config.imgdir_name.replace('[tmp]', str(img_size))\n",
    "\n",
    "DatasetCatalog.register(\n",
    "    f\"vinbigdata_test_{img_size}\", lambda: get_vinbigdata_dicts_test(imgdir_name, test_meta, debug=config.debug)\n",
    ")\n",
    "MetadataCatalog.get(f\"vinbigdata_test_{img_size}\").set(thing_classes=classes_nms)\n",
    "metadata_1024 = MetadataCatalog.get(f\"vinbigdata_test_{img_size}\")\n",
    "dataset_dicts_1024 = get_vinbigdata_dicts_test(\n",
    "    base_dir / imgdir_name, \n",
    "    test_meta,\n",
    "    test_data_type=f'png{img_size}',\n",
    "    debug=config.debug\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "local-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('faster_rcnn_R_101_FPN_3x_mkf_5_1024_rm', 'COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml', 1024, True),\n",
    "    ('faster_rcnn_R_50_FPN_3x_mkf_5_1024', 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml', 1024, True),\n",
    "    ('faster_rcnn_R_50_FPN_3x_mkf_5_1024_hmean', 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml', 1024, True),\n",
    "    ('retinanet_R_101_FPN_3x_mkf_5_1024', 'COCO-Detection/retinanet_R_101_FPN_3x.yaml', 1024, False),\n",
    "    ('retinanet_R_101_FPN_3x_mkf_5_1024_rm', 'COCO-Detection/retinanet_R_101_FPN_3x.yaml', 1024, False),\n",
    "    ('retinanet_R_101_FPN_3x_mkf_5_512', 'COCO-Detection/retinanet_R_101_FPN_3x.yaml', 512, False),\n",
    "    ('retinanet_R_50_FPN_3x_mkf_5_1024', 'COCO-Detection/retinanet_R_50_FPN_3x.yaml', 1024, False),\n",
    "    ('retinanet_R_50_FPN_3x_mkf_5_512', 'COCO-Detection/retinanet_R_50_FPN_3x.yaml', 512, False),\n",
    "    ('retinanet_R_50_FPN_3x_nms01_mkf_5_512', 'COCO-Detection/retinanet_R_50_FPN_3x.yaml', 512, False),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "protective-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset_dicts, batch_size, outdir, predictor, metadata):\n",
    "    results_list = []\n",
    "    index = 0\n",
    "\n",
    "    for i in tqdm(range(ceil(len(dataset_dicts) / batch_size))):\n",
    "        inds = list(range(batch_size * i, min(batch_size * (i + 1), len(dataset_dicts))))\n",
    "        dataset_dicts_batch = [dataset_dicts[i] for i in inds]\n",
    "        im_list = [cv2.imread(d[\"file_name\"]) for d in dataset_dicts_batch]\n",
    "        outputs_list = predict_batch(predictor, im_list)\n",
    "\n",
    "        for im, outputs, d in zip(im_list, outputs_list, dataset_dicts_batch):\n",
    "            resized_height, resized_width, ch = im.shape\n",
    "            outputs = predictor(im)\n",
    "            pred_classes = outputs[\"instances\"].to(\"cpu\").get_fields()['pred_classes'].numpy()\n",
    "            can_visualize = np.all((0 <= pred_classes) & (pred_classes <= len(classes_nms)))\n",
    "            if index < 5 and can_visualize:\n",
    "                # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "                v = Visualizer(\n",
    "                    im[:, :, ::-1],\n",
    "                    metadata=metadata,\n",
    "                    scale=0.5,\n",
    "                    instance_mode=ColorMode.IMAGE_BW\n",
    "                    # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "                )\n",
    "                out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "                cv2.imwrite(str(outdir / f\"pred_{index}.jpg\"), out.get_image()[:, :, ::-1])\n",
    "\n",
    "            image_id, dim0, dim1 = test_meta.iloc[index].values\n",
    "\n",
    "            instances = outputs[\"instances\"]\n",
    "            if len(instances) == 0:\n",
    "                # No finding, let's set 14 1 0 0 1 1x.\n",
    "                result = {\"image_id\": image_id, \"PredictionString\": \"14 1.0 0 0 1 1\"}\n",
    "            else:\n",
    "                # Find some bbox...\n",
    "                fields: Dict[str, Any] = instances.get_fields()\n",
    "                pred_classes = fields[\"pred_classes\"]  # (n_boxes,)\n",
    "                pred_scores = fields[\"scores\"]\n",
    "                # shape (n_boxes, 4). (xmin, ymin, xmax, ymax)\n",
    "                pred_boxes = fields[\"pred_boxes\"].tensor\n",
    "\n",
    "                h_ratio = dim0 / resized_height\n",
    "                w_ratio = dim1 / resized_width\n",
    "                pred_boxes[:, [0, 2]] *= w_ratio\n",
    "                pred_boxes[:, [1, 3]] *= h_ratio\n",
    "\n",
    "                pred_classes_array = pred_classes.cpu().numpy()\n",
    "                pred_boxes_array = pred_boxes.cpu().numpy()\n",
    "                pred_scores_array = pred_scores.cpu().numpy()\n",
    "\n",
    "                mask = (0 <= pred_classes_array) & (pred_classes_array < len(classes_nms))  # RetinaNet's outputs uncorrectly contain unknown class_id\n",
    "\n",
    "                result = {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"PredictionString\": format_pred(\n",
    "                        pred_classes_array[mask], pred_boxes_array[mask], pred_scores_array[mask]\n",
    "                    ),\n",
    "                }\n",
    "            results_list.append(result)\n",
    "            index += 1\n",
    "    \n",
    "    return pd.DataFrame(results_list, columns=['image_id', 'PredictionString'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-international",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11e9d9294954fbf8ccb962b4941f7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faster_rcnn_R_101_FPN_3x_mkf_5_1024_rm\n",
      "fold: 0\n",
      "cfg.OUTPUT_DIR ./output -> /home/yamaguchi-milkcocholate/VinBigData/src/VinBigData-ObjectDetection/detectron2_results/faster_rcnn_R_101_FPN_3x_mkf_5_1024_rm/fold-1\n",
      "Original thresh 0.05\n",
      "Changed  thresh 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0ed90d74e3492d94c7fb91c07fc021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.set_device(config.device_id)\n",
    "\n",
    "\n",
    "for model_dir, model_config, img_size, change_anchor in tqdm(models):\n",
    "    if model_dir not in ['faster_rcnn_R_101_FPN_3x_mkf_5_1024_rm', 'retinanet_R_101_FPN_3x_mkf_5_512']:\n",
    "        continue\n",
    "    print(model_dir)\n",
    "    outdir = base_dir / config.outdir.replace('[tmp]', model_dir)\n",
    "    dataset_nm = f'vinbigdata_test_{img_size}'\n",
    "    if img_size == 512:\n",
    "        dataset_dicts = dataset_dicts_512\n",
    "        metadata = metadata_512\n",
    "    elif img_size == 1024:\n",
    "        dataset_dicts = dataset_dicts_1024\n",
    "        metadata = metadata_1024\n",
    "    else:\n",
    "        raise ValueError()\n",
    "        \n",
    "    for fold in range(config.n_splits):\n",
    "        print(f'fold: {fold}')\n",
    "        seed_everything(seed=config.seed, device=config.device)\n",
    "\n",
    "        cfg = get_cfg()\n",
    "        original_output_dir = cfg.OUTPUT_DIR\n",
    "        cfg.OUTPUT_DIR = str(outdir / f'fold-{fold + 1}')\n",
    "        print(f\"cfg.OUTPUT_DIR {original_output_dir} -> {cfg.OUTPUT_DIR}\")\n",
    "\n",
    "        cfg.merge_from_file(model_zoo.get_config_file(model_config))\n",
    "        cfg.DATASETS.TRAIN = (\"vinbigdata_train\",)\n",
    "        cfg.DATASETS.TEST = ()\n",
    "\n",
    "        cfg.DATALOADER.NUM_WORKERS = config.num_workers\n",
    "        # Let training initialize from model zoo\n",
    "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config)\n",
    "        cfg.SOLVER.IMS_PER_BATCH = config.batch_size\n",
    "        cfg.SOLVER.BASE_LR = config.base_lr  # pick a good LR\n",
    "        cfg.SOLVER.MAX_ITER = config.iter\n",
    "        cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = config.roi_batch_size_per_image\n",
    "        cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes_nms)\n",
    "        \n",
    "        if change_anchor:\n",
    "            cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[16], [32], [64], [128], [256], [512], [1024]]\n",
    "            cfg.MODEL.RPN.IN_FEATURES = ['p2', 'p2', 'p3', 'p4', 'p5', 'p6', 'p6']\n",
    "            cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.33, 0.5, 1.0, 2.0, 3.0]]\n",
    "\n",
    "        # NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
    "\n",
    "        ### --- Inference & Evaluation ---\n",
    "        # Inference should use the config with parameters that are used in training\n",
    "        # cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "        # path to the model we just trained\n",
    "        cfg.MODEL.WEIGHTS = str(outdir / f'fold-{fold + 1}' / \"model_final.pth\")\n",
    "        print(\"Original thresh\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)  # 0.05\n",
    "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0  # set a custom testing threshold\n",
    "        print(\"Changed  thresh\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)\n",
    "        predictor = DefaultPredictor(cfg)\n",
    "\n",
    "        pred_df = inference(dataset_dicts=dataset_dicts, batch_size=config.batch_size, outdir=outdir / f'fold-{fold + 1}', predictor=predictor, metadata=metadata)\n",
    "        pred_df.to_csv(str(outdir / f'fold-{fold + 1}' / 'submmission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-brass",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
