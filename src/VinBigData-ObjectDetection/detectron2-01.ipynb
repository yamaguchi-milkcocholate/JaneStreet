{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efficient-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from typing import *\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pd.set_option('max_columns', 50)\n",
    "pd.set_option('max_rows', 200)\n",
    "warnings.simplefilter('ignore')\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mediterranean-theorem",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fail to import apex_C: apex was not installed or installed without --cpp_ext.\n",
      "fail to import amp_C: apex was not installed or installed without --cpp_ext.\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path().resolve()\n",
    "sys.path.append(str(base_dir / '../'))\n",
    "\n",
    "from utils.preprocess import *\n",
    "from utils.model import *\n",
    "from utils.train import *\n",
    "from utils.eval import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "protective-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int, device: str):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    with torch.cuda.device(device):\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acquired-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "import yaml\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # General\n",
    "    debug: bool = False\n",
    "    outdir: str = \"results01\"\n",
    "    device: str = \"cuda:1\"\n",
    "    device_id: int = 1\n",
    "    num_workers: int = 8\n",
    "\n",
    "    # Data config\n",
    "    imgconf_file: str = '../../data/VinBigData/train.csv'\n",
    "    meta_file: str = '../../data/VinBigData/train_meta.csv'\n",
    "    test_meta_file: str = '../../data/VinBigData/test_meta.csv'\n",
    "    imgdir_name: str = \"../../data/VinBigData/png1024\"\n",
    "    img_size: int = 1024\n",
    "    seed: int = 111\n",
    "    n_splits: int = 5\n",
    "    iou_thr: float = 0.5\n",
    "    skip_box_thr: float = 0.0001\n",
    "    \n",
    "    # Training config\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 8\n",
    "    \n",
    "    # Copy&Paste\n",
    "    iter: int = 10000\n",
    "    lr_scheduler_name: str = \"WarmupMultiStepLR\"  # WarmupMultiStepLR (default) or WarmupCosineLR\n",
    "    base_lr: float = 0.00025\n",
    "    roi_batch_size_per_image: int = 512\n",
    "    eval_period: int = 10000\n",
    "    checkpoint_period: int = 10000\n",
    "        \n",
    "    aug_kwargs: Dict = field(default_factory=lambda: {})\n",
    "        \n",
    "\n",
    "    def update(self, param_dict: Dict) -> \"Config\":\n",
    "        # Overwrite by `param_dict`\n",
    "        for key, value in param_dict.items():\n",
    "            if not hasattr(self, key):\n",
    "                raise ValueError(f\"[ERROR] Unexpected key for flag = {key}\")\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "    \n",
    "    def to_yaml(self, filepath: str, width: int = 120):\n",
    "        with open(filepath, 'w') as f:\n",
    "            yaml.dump(asdict(self), f, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enormous-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'debug': False,\n",
    "    'batch_size': 4,\n",
    "#     'patience': 10,\n",
    "    'iter': 1000,\n",
    "    'eval_period': 20,\n",
    "    'checkpoint_period': 250,\n",
    "    \"aug_kwargs\": {\n",
    "        \"HorizontalFlip\": {\"p\": 0.5},\n",
    "        \"ShiftScaleRotate\": {\"scale_limit\": 0.15, \"rotate_limit\": 10, \"p\": 0.5},\n",
    "        \"RandomBrightnessContrast\": {\"p\": 0.5},\n",
    "#         \"Normalize\": {\"mean\": (0.485, 0.456, 0.406), \"std\": (0.229, 0.224, 0.225)}\n",
    "    },\n",
    "}\n",
    "config = Config().update(config_dict)\n",
    "config.to_yaml(base_dir / config.outdir / 'config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "through-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_nms = [\n",
    "    \"Aortic enlargement\",\n",
    "    \"Atelectasis\",\n",
    "    \"Calcification\",\n",
    "    \"Cardiomegaly\",\n",
    "    \"Consolidation\",\n",
    "    \"ILD\",\n",
    "    \"Infiltration\",\n",
    "    \"Lung Opacity\",\n",
    "    \"Nodule/Mass\",\n",
    "    \"Other lesion\",\n",
    "    \"Pleural effusion\",\n",
    "    \"Pleural thickening\",\n",
    "    \"Pneumothorax\",\n",
    "    \"Pulmonary fibrosis\",\n",
    "#     \"No Finding\"\n",
    "]\n",
    "classes_dict = {index: class_name  for index, class_name in enumerate(classes_nms)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pleasant-semiconductor",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n",
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Methods for prediction for this competition\n",
    "from math import ceil\n",
    "\n",
    "import cv2\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "\n",
    "\n",
    "def format_pred(labels: np.ndarray, boxes: np.ndarray, scores: np.ndarray) -> str:\n",
    "    pred_strings = []\n",
    "    for label, score, bbox in zip(labels, scores, boxes):\n",
    "        xmin, ymin, xmax, ymax = bbox.astype(np.int64)\n",
    "        pred_strings.append(f\"{label} {score} {xmin} {ymin} {xmax} {ymax}\")\n",
    "    return \" \".join(pred_strings)\n",
    "\n",
    "\n",
    "def predict_batch(predictor: DefaultPredictor, im_list: List[np.ndarray]) -> List:\n",
    "    with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "        inputs_list = []\n",
    "        for original_image in im_list:\n",
    "            # Apply pre-processing to image.\n",
    "            if predictor.input_format == \"RGB\":\n",
    "                # whether the model expects BGR inputs or RGB\n",
    "                original_image = original_image[:, :, ::-1]\n",
    "            height, width = original_image.shape[:2]\n",
    "            # Do not apply original augmentation, which is resize.\n",
    "            # image = predictor.aug.get_transform(original_image).apply_image(original_image)\n",
    "            image = original_image\n",
    "            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "            inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "            inputs_list.append(inputs)\n",
    "        predictions = predictor.model(inputs_list)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hybrid-notion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Referenced:\n",
    " - https://detectron2.readthedocs.io/en/latest/tutorials/data_loading.html\n",
    " - https://www.kaggle.com/dhiiyaur/detectron-2-compare-models-augmentation/#data\n",
    "\"\"\"\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "import detectron2.data.transforms as T\n",
    "import torch\n",
    "from detectron2.data import detection_utils as utils\n",
    "\n",
    "\n",
    "class MyMapper:\n",
    "    \"\"\"Mapper which uses `detectron2.data.transforms` augmentations\"\"\"\n",
    "\n",
    "    def __init__(self, cfg, is_train: bool = True):\n",
    "        aug_kwargs = cfg.aug_kwargs\n",
    "        aug_list = [\n",
    "            # T.Resize((800, 800)),\n",
    "        ]\n",
    "        if is_train:\n",
    "            aug_list.extend([getattr(T, name)(**kwargs) for name, kwargs in aug_kwargs.items()])\n",
    "        self.augmentations = T.AugmentationList(aug_list)\n",
    "        self.is_train = is_train\n",
    "\n",
    "        mode = \"training\" if is_train else \"inference\"\n",
    "        print(f\"[MyDatasetMapper] Augmentations used in {mode}: {self.augmentations}\")\n",
    "\n",
    "    def __call__(self, dataset_dict):\n",
    "        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "        image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "\n",
    "        aug_input = T.AugInput(image)\n",
    "        transforms = self.augmentations(aug_input)\n",
    "        image = aug_input.image\n",
    "\n",
    "        # if not self.is_train:\n",
    "        #     # USER: Modify this if you want to keep them for some reason.\n",
    "        #     dataset_dict.pop(\"annotations\", None)\n",
    "        #     dataset_dict.pop(\"sem_seg_file_name\", None)\n",
    "        #     return dataset_dict\n",
    "\n",
    "        image_shape = image.shape[:2]  # h, w\n",
    "        dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "        annos = [\n",
    "            utils.transform_instance_annotations(obj, transforms, image_shape)\n",
    "            for obj in dataset_dict.pop(\"annotations\")\n",
    "            if obj.get(\"iscrowd\", 0) == 0\n",
    "        ]\n",
    "        instances = utils.annotations_to_instances(annos, image_shape)\n",
    "        dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "        return dataset_dict\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Referenced:\n",
    " - https://detectron2.readthedocs.io/en/latest/tutorials/data_loading.html\n",
    " - https://www.kaggle.com/dhiiyaur/detectron-2-compare-models-augmentation/#data\n",
    "\"\"\"\n",
    "import albumentations as A\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from detectron2.data import detection_utils as utils\n",
    "\n",
    "\n",
    "class AlbumentationsMapper:\n",
    "    \"\"\"Mapper which uses `albumentations` augmentations\"\"\"\n",
    "    def __init__(self, cfg, is_train: bool = True):\n",
    "        aug_kwargs = cfg.aug_kwargs\n",
    "        aug_list = [\n",
    "        ]\n",
    "        if is_train:\n",
    "            aug_list.extend([getattr(A, name)(**kwargs) for name, kwargs in aug_kwargs.items()])\n",
    "        self.transform = A.Compose(\n",
    "            aug_list, bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"category_ids\"])\n",
    "        )\n",
    "        self.is_train = is_train\n",
    "\n",
    "        mode = \"training\" if is_train else \"inference\"\n",
    "        print(f\"[AlbumentationsMapper] Augmentations used in {mode}: {self.transform}\")\n",
    "\n",
    "    def __call__(self, dataset_dict):\n",
    "        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "        image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "\n",
    "        # aug_input = T.AugInput(image)\n",
    "        # transforms = self.augmentations(aug_input)\n",
    "        # image = aug_input.image\n",
    "\n",
    "        prev_anno = dataset_dict[\"annotations\"]\n",
    "        bboxes = np.array([obj[\"bbox\"] for obj in prev_anno], dtype=np.float32)\n",
    "        # category_id = np.array([obj[\"category_id\"] for obj in dataset_dict[\"annotations\"]], dtype=np.int64)\n",
    "        category_id = np.arange(len(dataset_dict[\"annotations\"]))\n",
    "\n",
    "        transformed = self.transform(image=image, bboxes=bboxes, category_ids=category_id)\n",
    "        image = transformed[\"image\"]\n",
    "        annos = []\n",
    "        for i, j in enumerate(transformed[\"category_ids\"]):\n",
    "            d = prev_anno[j]\n",
    "            d[\"bbox\"] = transformed[\"bboxes\"][i]\n",
    "            annos.append(d)\n",
    "        dataset_dict.pop(\"annotations\", None)  # Remove unnecessary field.\n",
    "\n",
    "        # if not self.is_train:\n",
    "        #     # USER: Modify this if you want to keep them for some reason.\n",
    "        #     dataset_dict.pop(\"annotations\", None)\n",
    "        #     dataset_dict.pop(\"sem_seg_file_name\", None)\n",
    "        #     return dataset_dict\n",
    "\n",
    "        image_shape = image.shape[:2]  # h, w\n",
    "        dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "        instances = utils.annotations_to_instances(annos, image_shape)\n",
    "        dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "        return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceramic-candy",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HACKING: overriding COCOeval.summarize = vin_summarize...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Original code from https://github.com/cocodataset/cocoapi/blob/8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9/PythonAPI/pycocotools/cocoeval.py\n",
    "Just modified to show AP@40\n",
    "\"\"\"\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "import contextlib\n",
    "import copy\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import pycocotools.mask as mask_util\n",
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from tabulate import tabulate\n",
    "\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.config import CfgNode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.data.datasets.coco import convert_to_coco_json\n",
    "from detectron2.evaluation.evaluator import DatasetEvaluator\n",
    "from detectron2.evaluation.fast_eval_api import COCOeval_opt\n",
    "from detectron2.structures import Boxes, BoxMode, pairwise_iou\n",
    "from detectron2.utils.file_io import PathManager\n",
    "from detectron2.utils.logger import create_small_table\n",
    "\n",
    "\n",
    "def vin_summarize(self):\n",
    "    '''\n",
    "    Compute and display summary metrics for evaluation results.\n",
    "    Note this functin can *only* be applied on the default parameter setting\n",
    "    '''\n",
    "\n",
    "    def _summarize(ap=1, iouThr=None, areaRng='all', maxDets=100):\n",
    "        p = self.params\n",
    "        iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n",
    "        titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n",
    "        typeStr = '(AP)' if ap == 1 else '(AR)'\n",
    "        iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n",
    "            if iouThr is None else '{:0.2f}'.format(iouThr)\n",
    "\n",
    "        aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n",
    "        mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n",
    "        if ap == 1:\n",
    "            # dimension of precision: [TxRxKxAxM]\n",
    "            s = self.eval['precision']\n",
    "            # IoU\n",
    "            if iouThr is not None:\n",
    "                t = np.where(iouThr == p.iouThrs)[0]\n",
    "                s = s[t]\n",
    "            s = s[:, :, :, aind, mind]\n",
    "        else:\n",
    "            # dimension of recall: [TxKxAxM]\n",
    "            s = self.eval['recall']\n",
    "            if iouThr is not None:\n",
    "                t = np.where(iouThr == p.iouThrs)[0]\n",
    "                s = s[t]\n",
    "            s = s[:, :, aind, mind]\n",
    "        if len(s[s > -1]) == 0:\n",
    "            mean_s = -1\n",
    "        else:\n",
    "            mean_s = np.mean(s[s > -1])\n",
    "        print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n",
    "        return mean_s\n",
    "\n",
    "    def _summarizeDets():\n",
    "        stats = np.zeros((12,))\n",
    "        stats[0] = _summarize(1)\n",
    "        stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n",
    "        # stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n",
    "        stats[2] = _summarize(1, iouThr=.4, maxDets=self.params.maxDets[2])\n",
    "        stats[3] = _summarize(1, areaRng='small', maxDets=self.params.maxDets[2])\n",
    "        stats[4] = _summarize(1, areaRng='medium', maxDets=self.params.maxDets[2])\n",
    "        stats[5] = _summarize(1, areaRng='large', maxDets=self.params.maxDets[2])\n",
    "        stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n",
    "        stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n",
    "        stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n",
    "        stats[9] = _summarize(0, areaRng='small', maxDets=self.params.maxDets[2])\n",
    "        stats[10] = _summarize(0, areaRng='medium', maxDets=self.params.maxDets[2])\n",
    "        stats[11] = _summarize(0, areaRng='large', maxDets=self.params.maxDets[2])\n",
    "        return stats\n",
    "\n",
    "    def _summarizeKps():\n",
    "        stats = np.zeros((10,))\n",
    "        stats[0] = _summarize(1, maxDets=20)\n",
    "        stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n",
    "        stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n",
    "        stats[3] = _summarize(1, maxDets=20, areaRng='medium')\n",
    "        stats[4] = _summarize(1, maxDets=20, areaRng='large')\n",
    "        stats[5] = _summarize(0, maxDets=20)\n",
    "        stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n",
    "        stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n",
    "        stats[8] = _summarize(0, maxDets=20, areaRng='medium')\n",
    "        stats[9] = _summarize(0, maxDets=20, areaRng='large')\n",
    "        return stats\n",
    "\n",
    "    if not self.eval:\n",
    "        raise Exception('Please run accumulate() first')\n",
    "    iouType = self.params.iouType\n",
    "    if iouType == 'segm' or iouType == 'bbox':\n",
    "        summarize = _summarizeDets\n",
    "    elif iouType == 'keypoints':\n",
    "        summarize = _summarizeKps\n",
    "    self.stats = summarize()\n",
    "\n",
    "\n",
    "print(\"HACKING: overriding COCOeval.summarize = vin_summarize...\")\n",
    "COCOeval.summarize = vin_summarize\n",
    "\n",
    "\n",
    "class VinbigdataEvaluator(DatasetEvaluator):\n",
    "    \"\"\"\n",
    "    Evaluate AR for object proposals, AP for instance detection/segmentation, AP\n",
    "    for keypoint detection outputs using COCO's metrics.\n",
    "    See http://cocodataset.org/#detection-eval and\n",
    "    http://cocodataset.org/#keypoints-eval to understand its metrics.\n",
    "\n",
    "    In addition to COCO, this evaluator is able to support any bounding box detection,\n",
    "    instance segmentation, or keypoint detection dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        tasks=None,\n",
    "        distributed=True,\n",
    "        output_dir=None,\n",
    "        *,\n",
    "        use_fast_impl=True,\n",
    "        kpt_oks_sigmas=(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_name (str): name of the dataset to be evaluated.\n",
    "                It must have either the following corresponding metadata:\n",
    "\n",
    "                    \"json_file\": the path to the COCO format annotation\n",
    "\n",
    "                Or it must be in detectron2's standard dataset format\n",
    "                so it can be converted to COCO format automatically.\n",
    "            tasks (tuple[str]): tasks that can be evaluated under the given\n",
    "                configuration. A task is one of \"bbox\", \"segm\", \"keypoints\".\n",
    "                By default, will infer this automatically from predictions.\n",
    "            distributed (True): if True, will collect results from all ranks and run evaluation\n",
    "                in the main process.\n",
    "                Otherwise, will only evaluate the results in the current process.\n",
    "            output_dir (str): optional, an output directory to dump all\n",
    "                results predicted on the dataset. The dump contains two files:\n",
    "\n",
    "                1. \"instances_predictions.pth\" a file in torch serialization\n",
    "                   format that contains all the raw original predictions.\n",
    "                2. \"coco_instances_results.json\" a json file in COCO's result\n",
    "                   format.\n",
    "            use_fast_impl (bool): use a fast but **unofficial** implementation to compute AP.\n",
    "                Although the results should be very close to the official implementation in COCO\n",
    "                API, it is still recommended to compute results with the official API for use in\n",
    "                papers. The faster implementation also uses more RAM.\n",
    "            kpt_oks_sigmas (list[float]): The sigmas used to calculate keypoint OKS.\n",
    "                See http://cocodataset.org/#keypoints-eval\n",
    "                When empty, it will use the defaults in COCO.\n",
    "                Otherwise it should be the same length as ROI_KEYPOINT_HEAD.NUM_KEYPOINTS.\n",
    "        \"\"\"\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "        self._distributed = distributed\n",
    "        self._output_dir = output_dir\n",
    "        self._use_fast_impl = use_fast_impl\n",
    "\n",
    "        if tasks is not None and isinstance(tasks, CfgNode):\n",
    "            kpt_oks_sigmas = (\n",
    "                tasks.TEST.KEYPOINT_OKS_SIGMAS if not kpt_oks_sigmas else kpt_oks_sigmas\n",
    "            )\n",
    "            self._logger.warn(\n",
    "                \"COCO Evaluator instantiated using config, this is deprecated behavior.\"\n",
    "                \" Please pass in explicit arguments instead.\"\n",
    "            )\n",
    "            self._tasks = None  # Infering it from predictions should be better\n",
    "        else:\n",
    "            self._tasks = tasks\n",
    "\n",
    "        self._cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "        self._metadata = MetadataCatalog.get(dataset_name)\n",
    "        if not hasattr(self._metadata, \"json_file\"):\n",
    "            self._logger.info(\n",
    "                f\"'{dataset_name}' is not registered by `register_coco_instances`.\"\n",
    "                \" Therefore trying to convert it to COCO format ...\"\n",
    "            )\n",
    "\n",
    "            cache_path = os.path.join(output_dir, f\"{dataset_name}_coco_format.json\")\n",
    "            self._metadata.json_file = cache_path\n",
    "            convert_to_coco_json(dataset_name, cache_path)\n",
    "\n",
    "        json_file = PathManager.get_local_path(self._metadata.json_file)\n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            self._coco_api = COCO(json_file)\n",
    "\n",
    "        # Test set json files do not contain annotations (evaluation must be\n",
    "        # performed using the COCO evaluation server).\n",
    "        self._do_evaluation = \"annotations\" in self._coco_api.dataset\n",
    "        if self._do_evaluation:\n",
    "            self._kpt_oks_sigmas = kpt_oks_sigmas\n",
    "\n",
    "    def reset(self):\n",
    "        self._predictions = []\n",
    "\n",
    "    def process(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).\n",
    "                It is a list of dict. Each dict corresponds to an image and\n",
    "                contains keys like \"height\", \"width\", \"file_name\", \"image_id\".\n",
    "            outputs: the outputs of a COCO model. It is a list of dicts with key\n",
    "                \"instances\" that contains :class:`Instances`.\n",
    "        \"\"\"\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            prediction = {\"image_id\": input[\"image_id\"]}\n",
    "\n",
    "            if \"instances\" in output:\n",
    "                instances = output[\"instances\"].to(self._cpu_device)\n",
    "                prediction[\"instances\"] = instances_to_coco_json(instances, input[\"image_id\"])\n",
    "            if \"proposals\" in output:\n",
    "                prediction[\"proposals\"] = output[\"proposals\"].to(self._cpu_device)\n",
    "            if len(prediction) > 1:\n",
    "                self._predictions.append(prediction)\n",
    "\n",
    "    def evaluate(self, img_ids=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_ids: a list of image IDs to evaluate on. Default to None for the whole dataset\n",
    "        \"\"\"\n",
    "        if self._distributed:\n",
    "            comm.synchronize()\n",
    "            predictions = comm.gather(self._predictions, dst=0)\n",
    "            predictions = list(itertools.chain(*predictions))\n",
    "\n",
    "            if not comm.is_main_process():\n",
    "                return {}\n",
    "        else:\n",
    "            predictions = self._predictions\n",
    "\n",
    "        if len(predictions) == 0:\n",
    "            self._logger.warning(\"[VinbigdataEvaluator] Did not receive valid predictions.\")\n",
    "            return {}\n",
    "\n",
    "        if self._output_dir:\n",
    "            PathManager.mkdirs(self._output_dir)\n",
    "            file_path = os.path.join(self._output_dir, \"instances_predictions.pth\")\n",
    "            with PathManager.open(file_path, \"wb\") as f:\n",
    "                torch.save(predictions, f)\n",
    "\n",
    "        self._results = OrderedDict()\n",
    "        if \"proposals\" in predictions[0]:\n",
    "            self._eval_box_proposals(predictions)\n",
    "        if \"instances\" in predictions[0]:\n",
    "            self._eval_predictions(predictions, img_ids=img_ids)\n",
    "        # Copy so the caller can do whatever with results\n",
    "        return copy.deepcopy(self._results)\n",
    "\n",
    "    def _tasks_from_predictions(self, predictions):\n",
    "        \"\"\"\n",
    "        Get COCO API \"tasks\" (i.e. iou_type) from COCO-format predictions.\n",
    "        \"\"\"\n",
    "        tasks = {\"bbox\"}\n",
    "        for pred in predictions:\n",
    "            if \"segmentation\" in pred:\n",
    "                tasks.add(\"segm\")\n",
    "            if \"keypoints\" in pred:\n",
    "                tasks.add(\"keypoints\")\n",
    "        return sorted(tasks)\n",
    "\n",
    "    def _eval_predictions(self, predictions, img_ids=None):\n",
    "        \"\"\"\n",
    "        Evaluate predictions. Fill self._results with the metrics of the tasks.\n",
    "        \"\"\"\n",
    "        self._logger.info(\"Preparing results for COCO format ...\")\n",
    "        coco_results = list(itertools.chain(*[x[\"instances\"] for x in predictions]))\n",
    "        tasks = self._tasks or self._tasks_from_predictions(coco_results)\n",
    "\n",
    "        # unmap the category ids for COCO\n",
    "        if hasattr(self._metadata, \"thing_dataset_id_to_contiguous_id\"):\n",
    "            dataset_id_to_contiguous_id = self._metadata.thing_dataset_id_to_contiguous_id\n",
    "            all_contiguous_ids = list(dataset_id_to_contiguous_id.values())\n",
    "            num_classes = len(all_contiguous_ids)\n",
    "            assert min(all_contiguous_ids) == 0 and max(all_contiguous_ids) == num_classes - 1\n",
    "\n",
    "            reverse_id_mapping = {v: k for k, v in dataset_id_to_contiguous_id.items()}\n",
    "            for result in coco_results:\n",
    "                category_id = result[\"category_id\"]\n",
    "                assert category_id < num_classes, (\n",
    "                    f\"A prediction has class={category_id}, \"\n",
    "                    f\"but the dataset only has {num_classes} classes and \"\n",
    "                    f\"predicted class id should be in [0, {num_classes - 1}].\"\n",
    "                )\n",
    "                result[\"category_id\"] = reverse_id_mapping[category_id]\n",
    "\n",
    "        if self._output_dir:\n",
    "            file_path = os.path.join(self._output_dir, \"coco_instances_results.json\")\n",
    "            self._logger.info(\"Saving results to {}\".format(file_path))\n",
    "            with PathManager.open(file_path, \"w\") as f:\n",
    "                f.write(json.dumps(coco_results))\n",
    "                f.flush()\n",
    "\n",
    "        if not self._do_evaluation:\n",
    "            self._logger.info(\"Annotations are not available for evaluation.\")\n",
    "            return\n",
    "\n",
    "        self._logger.info(\n",
    "            \"Evaluating predictions with {} COCO API...\".format(\n",
    "                \"unofficial\" if self._use_fast_impl else \"official\"\n",
    "            )\n",
    "        )\n",
    "        for task in sorted(tasks):\n",
    "            coco_eval = (\n",
    "                _evaluate_predictions_on_coco(\n",
    "                    self._coco_api,\n",
    "                    coco_results,\n",
    "                    task,\n",
    "                    kpt_oks_sigmas=self._kpt_oks_sigmas,\n",
    "                    use_fast_impl=self._use_fast_impl,\n",
    "                    img_ids=img_ids,\n",
    "                )\n",
    "                if len(coco_results) > 0\n",
    "                else None  # cocoapi does not handle empty results very well\n",
    "            )\n",
    "\n",
    "            res = self._derive_coco_results(\n",
    "                coco_eval, task, class_names=self._metadata.get(\"thing_classes\")\n",
    "            )\n",
    "            self._results[task] = res\n",
    "\n",
    "    def _eval_box_proposals(self, predictions):\n",
    "        \"\"\"\n",
    "        Evaluate the box proposals in predictions.\n",
    "        Fill self._results with the metrics for \"box_proposals\" task.\n",
    "        \"\"\"\n",
    "        if self._output_dir:\n",
    "            # Saving generated box proposals to file.\n",
    "            # Predicted box_proposals are in XYXY_ABS mode.\n",
    "            bbox_mode = BoxMode.XYXY_ABS.value\n",
    "            ids, boxes, objectness_logits = [], [], []\n",
    "            for prediction in predictions:\n",
    "                ids.append(prediction[\"image_id\"])\n",
    "                boxes.append(prediction[\"proposals\"].proposal_boxes.tensor.numpy())\n",
    "                objectness_logits.append(prediction[\"proposals\"].objectness_logits.numpy())\n",
    "\n",
    "            proposal_data = {\n",
    "                \"boxes\": boxes,\n",
    "                \"objectness_logits\": objectness_logits,\n",
    "                \"ids\": ids,\n",
    "                \"bbox_mode\": bbox_mode,\n",
    "            }\n",
    "            with PathManager.open(os.path.join(self._output_dir, \"box_proposals.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(proposal_data, f)\n",
    "\n",
    "        if not self._do_evaluation:\n",
    "            self._logger.info(\"Annotations are not available for evaluation.\")\n",
    "            return\n",
    "\n",
    "        self._logger.info(\"Evaluating bbox proposals ...\")\n",
    "        res = {}\n",
    "        areas = {\"all\": \"\", \"small\": \"s\", \"medium\": \"m\", \"large\": \"l\"}\n",
    "        for limit in [100, 1000]:\n",
    "            for area, suffix in areas.items():\n",
    "                stats = _evaluate_box_proposals(predictions, self._coco_api, area=area, limit=limit)\n",
    "                key = \"AR{}@{:d}\".format(suffix, limit)\n",
    "                res[key] = float(stats[\"ar\"].item() * 100)\n",
    "        self._logger.info(\"Proposal metrics: \\n\" + create_small_table(res))\n",
    "        self._results[\"box_proposals\"] = res\n",
    "\n",
    "    def _derive_coco_results(self, coco_eval, iou_type, class_names=None):\n",
    "        \"\"\"\n",
    "        Derive the desired score numbers from summarized COCOeval.\n",
    "\n",
    "        Args:\n",
    "            coco_eval (None or COCOEval): None represents no predictions from model.\n",
    "            iou_type (str):\n",
    "            class_names (None or list[str]): if provided, will use it to predict\n",
    "                per-category AP.\n",
    "\n",
    "        Returns:\n",
    "            a dict of {metric name: score}\n",
    "        \"\"\"\n",
    "\n",
    "        metrics = {\n",
    "            \"bbox\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
    "            \"segm\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
    "            \"keypoints\": [\"AP\", \"AP50\", \"AP75\", \"APm\", \"APl\"],\n",
    "        }[iou_type]\n",
    "\n",
    "        if coco_eval is None:\n",
    "            self._logger.warn(\"No predictions from the model!\")\n",
    "            return {metric: float(\"nan\") for metric in metrics}\n",
    "\n",
    "        # the standard metrics\n",
    "        results = {\n",
    "            metric: float(coco_eval.stats[idx] * 100 if coco_eval.stats[idx] >= 0 else \"nan\")\n",
    "            for idx, metric in enumerate(metrics)\n",
    "        }\n",
    "        self._logger.info(\n",
    "            \"Evaluation results for {}: \\n\".format(iou_type) + create_small_table(results)\n",
    "        )\n",
    "        if not np.isfinite(sum(results.values())):\n",
    "            self._logger.info(\"Some metrics cannot be computed and is shown as NaN.\")\n",
    "\n",
    "        if class_names is None or len(class_names) <= 1:\n",
    "            return results\n",
    "        # Compute per-category AP\n",
    "        # from https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L222-L252 # noqa\n",
    "        precisions = coco_eval.eval[\"precision\"]\n",
    "        # precision has dims (iou, recall, cls, area range, max dets)\n",
    "        assert len(class_names) == precisions.shape[2]\n",
    "\n",
    "        results_per_category = []\n",
    "        for idx, name in enumerate(class_names):\n",
    "            # area range index 0: all area ranges\n",
    "            # max dets index -1: typically 100 per image\n",
    "            precision = precisions[:, :, idx, 0, -1]\n",
    "            precision = precision[precision > -1]\n",
    "            ap = np.mean(precision) if precision.size else float(\"nan\")\n",
    "            results_per_category.append((\"{}\".format(name), float(ap * 100)))\n",
    "\n",
    "        # tabulate it\n",
    "        N_COLS = min(6, len(results_per_category) * 2)\n",
    "        results_flatten = list(itertools.chain(*results_per_category))\n",
    "        results_2d = itertools.zip_longest(*[results_flatten[i::N_COLS] for i in range(N_COLS)])\n",
    "        table = tabulate(\n",
    "            results_2d,\n",
    "            tablefmt=\"pipe\",\n",
    "            floatfmt=\".3f\",\n",
    "            headers=[\"category\", \"AP\"] * (N_COLS // 2),\n",
    "            numalign=\"left\",\n",
    "        )\n",
    "        self._logger.info(\"Per-category {} AP: \\n\".format(iou_type) + table)\n",
    "\n",
    "        results.update({\"AP-\" + name: ap for name, ap in results_per_category})\n",
    "        return results\n",
    "\n",
    "\n",
    "def instances_to_coco_json(instances, img_id):\n",
    "    \"\"\"\n",
    "    Dump an \"Instances\" object to a COCO-format json that's used for evaluation.\n",
    "\n",
    "    Args:\n",
    "        instances (Instances):\n",
    "        img_id (int): the image id\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: list of json annotations in COCO format.\n",
    "    \"\"\"\n",
    "    num_instance = len(instances)\n",
    "    if num_instance == 0:\n",
    "        return []\n",
    "\n",
    "    boxes = instances.pred_boxes.tensor.numpy()\n",
    "    boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)\n",
    "    boxes = boxes.tolist()\n",
    "    scores = instances.scores.tolist()\n",
    "    classes = instances.pred_classes.tolist()\n",
    "\n",
    "    has_mask = instances.has(\"pred_masks\")\n",
    "    if has_mask:\n",
    "        # use RLE to encode the masks, because they are too large and takes memory\n",
    "        # since this evaluator stores outputs of the entire dataset\n",
    "        rles = [\n",
    "            mask_util.encode(np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\"))[0]\n",
    "            for mask in instances.pred_masks\n",
    "        ]\n",
    "        for rle in rles:\n",
    "            # \"counts\" is an array encoded by mask_util as a byte-stream. Python3's\n",
    "            # json writer which always produces strings cannot serialize a bytestream\n",
    "            # unless you decode it. Thankfully, utf-8 works out (which is also what\n",
    "            # the pycocotools/_mask.pyx does).\n",
    "            rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
    "\n",
    "    has_keypoints = instances.has(\"pred_keypoints\")\n",
    "    if has_keypoints:\n",
    "        keypoints = instances.pred_keypoints\n",
    "\n",
    "    results = []\n",
    "    for k in range(num_instance):\n",
    "        result = {\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": classes[k],\n",
    "            \"bbox\": boxes[k],\n",
    "            \"score\": scores[k],\n",
    "        }\n",
    "        if has_mask:\n",
    "            result[\"segmentation\"] = rles[k]\n",
    "        if has_keypoints:\n",
    "            # In COCO annotations,\n",
    "            # keypoints coordinates are pixel indices.\n",
    "            # However our predictions are floating point coordinates.\n",
    "            # Therefore we subtract 0.5 to be consistent with the annotation format.\n",
    "            # This is the inverse of data loading logic in `datasets/coco.py`.\n",
    "            keypoints[k][:, :2] -= 0.5\n",
    "            result[\"keypoints\"] = keypoints[k].flatten().tolist()\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "# inspired from Detectron:\n",
    "# https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L255 # noqa\n",
    "def _evaluate_box_proposals(dataset_predictions, coco_api, thresholds=None, area=\"all\", limit=None):\n",
    "    \"\"\"\n",
    "    Evaluate detection proposal recall metrics. This function is a much\n",
    "    faster alternative to the official COCO API recall evaluation code. However,\n",
    "    it produces slightly different results.\n",
    "    \"\"\"\n",
    "    # Record max overlap value for each gt box\n",
    "    # Return vector of overlap values\n",
    "    areas = {\n",
    "        \"all\": 0,\n",
    "        \"small\": 1,\n",
    "        \"medium\": 2,\n",
    "        \"large\": 3,\n",
    "        \"96-128\": 4,\n",
    "        \"128-256\": 5,\n",
    "        \"256-512\": 6,\n",
    "        \"512-inf\": 7,\n",
    "    }\n",
    "    area_ranges = [\n",
    "        [0 ** 2, 1e5 ** 2],  # all\n",
    "        [0 ** 2, 32 ** 2],  # small\n",
    "        [32 ** 2, 96 ** 2],  # medium\n",
    "        [96 ** 2, 1e5 ** 2],  # large\n",
    "        [96 ** 2, 128 ** 2],  # 96-128\n",
    "        [128 ** 2, 256 ** 2],  # 128-256\n",
    "        [256 ** 2, 512 ** 2],  # 256-512\n",
    "        [512 ** 2, 1e5 ** 2],\n",
    "    ]  # 512-inf\n",
    "    assert area in areas, \"Unknown area range: {}\".format(area)\n",
    "    area_range = area_ranges[areas[area]]\n",
    "    gt_overlaps = []\n",
    "    num_pos = 0\n",
    "\n",
    "    for prediction_dict in dataset_predictions:\n",
    "        predictions = prediction_dict[\"proposals\"]\n",
    "\n",
    "        # sort predictions in descending order\n",
    "        # TODO maybe remove this and make it explicit in the documentation\n",
    "        inds = predictions.objectness_logits.sort(descending=True)[1]\n",
    "        predictions = predictions[inds]\n",
    "\n",
    "        ann_ids = coco_api.getAnnIds(imgIds=prediction_dict[\"image_id\"])\n",
    "        anno = coco_api.loadAnns(ann_ids)\n",
    "        gt_boxes = [\n",
    "            BoxMode.convert(obj[\"bbox\"], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
    "            for obj in anno\n",
    "            if obj[\"iscrowd\"] == 0\n",
    "        ]\n",
    "        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes\n",
    "        gt_boxes = Boxes(gt_boxes)\n",
    "        gt_areas = torch.as_tensor([obj[\"area\"] for obj in anno if obj[\"iscrowd\"] == 0])\n",
    "\n",
    "        if len(gt_boxes) == 0 or len(predictions) == 0:\n",
    "            continue\n",
    "\n",
    "        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])\n",
    "        gt_boxes = gt_boxes[valid_gt_inds]\n",
    "\n",
    "        num_pos += len(gt_boxes)\n",
    "\n",
    "        if len(gt_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        if limit is not None and len(predictions) > limit:\n",
    "            predictions = predictions[:limit]\n",
    "\n",
    "        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)\n",
    "\n",
    "        _gt_overlaps = torch.zeros(len(gt_boxes))\n",
    "        for j in range(min(len(predictions), len(gt_boxes))):\n",
    "            # find which proposal box maximally covers each gt box\n",
    "            # and get the iou amount of coverage for each gt box\n",
    "            max_overlaps, argmax_overlaps = overlaps.max(dim=0)\n",
    "\n",
    "            # find which gt box is 'best' covered (i.e. 'best' = most iou)\n",
    "            gt_ovr, gt_ind = max_overlaps.max(dim=0)\n",
    "            assert gt_ovr >= 0\n",
    "            # find the proposal box that covers the best covered gt box\n",
    "            box_ind = argmax_overlaps[gt_ind]\n",
    "            # record the iou coverage of this gt box\n",
    "            _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n",
    "            assert _gt_overlaps[j] == gt_ovr\n",
    "            # mark the proposal box and the gt box as used\n",
    "            overlaps[box_ind, :] = -1\n",
    "            overlaps[:, gt_ind] = -1\n",
    "\n",
    "        # append recorded iou coverage level\n",
    "        gt_overlaps.append(_gt_overlaps)\n",
    "    gt_overlaps = (\n",
    "        torch.cat(gt_overlaps, dim=0) if len(gt_overlaps) else torch.zeros(0, dtype=torch.float32)\n",
    "    )\n",
    "    gt_overlaps, _ = torch.sort(gt_overlaps)\n",
    "\n",
    "    if thresholds is None:\n",
    "        step = 0.05\n",
    "        # thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)\n",
    "        thresholds = torch.arange(0.4, 0.95 + 1e-5, step, dtype=torch.float32)\n",
    "    recalls = torch.zeros_like(thresholds)\n",
    "    # compute recall for each iou threshold\n",
    "    for i, t in enumerate(thresholds):\n",
    "        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)\n",
    "    # ar = 2 * np.trapz(recalls, thresholds)\n",
    "    ar = recalls.mean()\n",
    "    return {\n",
    "        \"ar\": ar,\n",
    "        \"recalls\": recalls,\n",
    "        \"thresholds\": thresholds,\n",
    "        \"gt_overlaps\": gt_overlaps,\n",
    "        \"num_pos\": num_pos,\n",
    "    }\n",
    "\n",
    "\n",
    "def _evaluate_predictions_on_coco(\n",
    "    coco_gt, coco_results, iou_type, kpt_oks_sigmas=None, use_fast_impl=True, img_ids=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the coco results using COCOEval API.\n",
    "    \"\"\"\n",
    "    assert len(coco_results) > 0\n",
    "\n",
    "    if iou_type == \"segm\":\n",
    "        coco_results = copy.deepcopy(coco_results)\n",
    "        # When evaluating mask AP, if the results contain bbox, cocoapi will\n",
    "        # use the box area as the area of the instance, instead of the mask area.\n",
    "        # This leads to a different definition of small/medium/large.\n",
    "        # We remove the bbox field to let mask AP use mask area.\n",
    "        for c in coco_results:\n",
    "            c.pop(\"bbox\", None)\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(coco_results)\n",
    "    coco_eval = (COCOeval_opt if use_fast_impl else COCOeval)(coco_gt, coco_dt, iou_type)\n",
    "\n",
    "    # HACKING: overwrite iouThrs to calc ious 0.4\n",
    "    coco_eval.params.iouThrs = np.linspace(\n",
    "        .4, 0.95, int(np.round((0.95 - .4) / .05)) + 1, endpoint=True)\n",
    "\n",
    "    if img_ids is not None:\n",
    "        coco_eval.params.imgIds = img_ids\n",
    "\n",
    "    if iou_type == \"keypoints\":\n",
    "        # Use the COCO default keypoint OKS sigmas unless overrides are specified\n",
    "        if kpt_oks_sigmas:\n",
    "            assert hasattr(coco_eval.params, \"kpt_oks_sigmas\"), \"pycocotools is too old!\"\n",
    "            coco_eval.params.kpt_oks_sigmas = np.array(kpt_oks_sigmas)\n",
    "        # COCOAPI requires every detection and every gt to have keypoints, so\n",
    "        # we just take the first entry from both\n",
    "        num_keypoints_dt = len(coco_results[0][\"keypoints\"]) // 3\n",
    "        num_keypoints_gt = len(next(iter(coco_gt.anns.values()))[\"keypoints\"]) // 3\n",
    "        num_keypoints_oks = len(coco_eval.params.kpt_oks_sigmas)\n",
    "        assert num_keypoints_oks == num_keypoints_dt == num_keypoints_gt, (\n",
    "            f\"[VinbigdataEvaluator] Prediction contain {num_keypoints_dt} keypoints. \"\n",
    "            f\"Ground truth contains {num_keypoints_gt} keypoints. \"\n",
    "            f\"The length of cfg.TEST.KEYPOINT_OKS_SIGMAS is {num_keypoints_oks}. \"\n",
    "            \"They have to agree with each other. For meaning of OKS, please refer to \"\n",
    "            \"http://cocodataset.org/#keypoints-eval.\"\n",
    "        )\n",
    "\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    return coco_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "attempted-acrylic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To calculate & record validation loss\n",
    "\n",
    "Original code from https://medium.com/@apofeniaco/training-on-detectron2-with-a-validation-set-and-plot-loss-on-it-to-avoid-overfitting-6449418fbf4e\n",
    "by @apofeniaco\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "import detectron2.utils.comm as comm\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "\n",
    "    def _do_loss_eval(self):\n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):\n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)\n",
    "                    ),\n",
    "                    n=5,\n",
    "                )\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        # self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        comm.synchronize()\n",
    "\n",
    "        # return losses\n",
    "        return mean_loss\n",
    "\n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop\n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "\n",
    "    def after_step(self):\n",
    "        next_iter = int(self.trainer.iter) + 1\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            mean_loss = self._do_loss_eval()\n",
    "            self.trainer.storage.put_scalars(validation_loss=mean_loss)\n",
    "            print(\"validation do loss eval\", mean_loss)\n",
    "        else:\n",
    "            pass\n",
    "            # self.trainer.storage.put_scalars(timetest=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "industrial-statement",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer, launch\n",
    "\n",
    "# from detectron2.evaluation import COCOEvaluator, PascalVOCDetectionEvaluator\n",
    "\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg, sampler=None):\n",
    "        return build_detection_train_loader(\n",
    "            cfg, mapper=AlbumentationsMapper(cfg, True), sampler=sampler\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        return build_detection_test_loader(\n",
    "            cfg, dataset_name, mapper=AlbumentationsMapper(cfg, False)\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        # return PascalVOCDetectionEvaluator(dataset_name)  # not working\n",
    "        # return COCOEvaluator(dataset_name, (\"bbox\",), False, output_dir=output_folder)\n",
    "        return VinbigdataEvaluator(dataset_name, (\"bbox\",), False, output_dir=output_folder)\n",
    "\n",
    "    def build_hooks(self):\n",
    "        hooks = super(MyTrainer, self).build_hooks()\n",
    "        cfg = self.cfg\n",
    "        if len(cfg.DATASETS.TEST) > 0:\n",
    "            loss_eval_hook = LossEvalHook(\n",
    "                cfg.TEST.EVAL_PERIOD,\n",
    "                self.model,\n",
    "                MyTrainer.build_test_loader(cfg, cfg.DATASETS.TEST[0]),\n",
    "            )\n",
    "            hooks.insert(-1, loss_eval_hook)\n",
    "\n",
    "        return hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "asian-spice",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger detectron2 (DEBUG)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import dataclasses\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import cv2\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer, launch\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "devoted-southeast",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_train_data(filepath: str, meta_filepath: str, img_size: int) -> pd.DataFrame:\n",
    "    train = pd.read_csv(filepath)\n",
    "    train.fillna(0, inplace=True)\n",
    "    train.loc[train[\"class_id\"] == 14, ['x_max', 'y_max']] = 1.0\n",
    "    \n",
    "    train_meta = pd.read_csv(meta_filepath)\n",
    "    \n",
    "    train = pd.merge(train, train_meta, how='left', on='image_id')\n",
    "    \n",
    "    train[f'x_min_{img_size}'] = (img_size / train['dim1'] * train['x_min'])\n",
    "    train[f'x_max_{img_size}'] = (img_size / train['dim1'] * train['x_max'])\n",
    "    train[f'y_min_{img_size}'] = (img_size / train['dim0'] * train['y_min'])\n",
    "    train[f'y_max_{img_size}'] = (img_size / train['dim0'] * train['y_max'])\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "solved-printer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ensemble_boxes import *\n",
    "\n",
    "\n",
    "def change_wbf(dataset_dict: Dict[str, Any], iou_thr: float, skip_box_thr: float, n_cls: int):\n",
    "    cls_ids = list(range(n_cls))\n",
    "    \n",
    "    height, width = dataset_dict['height'], dataset_dict['width']\n",
    "    \n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    scores = [1.0] * len(dataset_dict['annotations'])\n",
    "        \n",
    "    for obj in dataset_dict['annotations']:\n",
    "        box = [\n",
    "            obj['bbox'][0] / width,\n",
    "            obj['bbox'][1] / height,\n",
    "            obj['bbox'][2] / width,\n",
    "            obj['bbox'][3] / height\n",
    "        ]\n",
    "        boxes += [box]\n",
    "        labels += [obj['category_id']]\n",
    "    \n",
    "    boxes, scores, labels = weighted_boxes_fusion([boxes], [scores], [labels], weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    \n",
    "    annots = list()\n",
    "    for box_, label_ in zip(boxes, labels):\n",
    "        annots += [{\n",
    "            'bbox': [\n",
    "                box_[0] * width,\n",
    "                box_[1] * height,\n",
    "                box_[2] * width,\n",
    "                box_[3] * height\n",
    "            ],\n",
    "            'category_id': int(label_),\n",
    "            'bbox_mode': BoxMode.XYXY_ABS,\n",
    "        }]\n",
    "        \n",
    "    return {\n",
    "        'file_name': dataset_dict['file_name'],\n",
    "        'image_id': dataset_dict['image_id'],\n",
    "        'height': height,\n",
    "        'width': width,\n",
    "        'annotations': annots\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "superior-heather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing class_id = 14: 67914  36096\n"
     ]
    }
   ],
   "source": [
    "train = load_train_data(filepath=str(base_dir / config.imgconf_file), meta_filepath=str(base_dir / config.meta_file), img_size=config.img_size)\n",
    "mkf = MultilabelKFoldWrapper(train, n_splits=config.n_splits, seed=config.seed, remove_normal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "intended-financing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "image shape: (1024, 1024, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3c75904be64ed195dfae75fecbd532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from cache dataset_dicts_cache_png1024_debug0.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset_dicts = get_vinbigdata_dicts(\n",
    "    imgdir=base_dir / config.imgdir_name,\n",
    "    meta_filepath=base_dir / config.meta_file,\n",
    "    train_df=train,\n",
    "    train_data_type=f'png{config.img_size}',\n",
    "    debug=config.debug,\n",
    ")\n",
    "\n",
    "# add annotation into normal images\n",
    "#     for dd in dataset_dicts:\n",
    "#         if len(dd['annotations']) == 0:\n",
    "#             dd['annotations'] = [{\n",
    "#                 'bbox': [0, 0, 1, 1],\n",
    "#                 'bbox_mode': None,\n",
    "#                 'category_id': 14\n",
    "#             }]\n",
    "dataset_dicts = [dd for dd in dataset_dicts if len(dd['annotations']) > 0]\n",
    "\n",
    "# label cleaning\n",
    "dataset_dicts = [change_wbf(dd, iou_thr=config.iou_thr, skip_box_thr=config.skip_box_thr, n_cls=len(classes_nms)) for dd in dataset_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "recognized-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "train_df, valid_df = mkf[fold]\n",
    "\n",
    "train_idx = list(pd.DataFrame({'image_id': [dd['image_id'] for dd in dataset_dicts]}).query(f\"image_id in {train_df['image_id'].values.tolist()}\").index)\n",
    "valid_idx = list(pd.DataFrame({'image_id': [dd['image_id'] for dd in dataset_dicts]}).query(f\"image_id in {valid_df['image_id'].values.tolist()}\").index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "joint-cherry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DatasetCatalog.register(\n",
    "    \"vinbigdata_train\",\n",
    "    lambda: [dataset_dicts[i] for i in train_idx],\n",
    ")\n",
    "MetadataCatalog.get(\"vinbigdata_train\").set(thing_classes=classes_nms)\n",
    "\n",
    "DatasetCatalog.register(\n",
    "    \"vinbigdata_valid\",\n",
    "    lambda: [dataset_dicts[i] for i in valid_idx],\n",
    ")\n",
    "MetadataCatalog.get(\"vinbigdata_valid\").set(thing_classes=classes_nms)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "universal-cargo",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize data...\n",
    "# anomaly_image_ids = train.query(\"class_id != 14\")[\"image_id\"].unique()\n",
    "# train_meta = pd.read_csv(base_dir / config.imgdir_name / \"train_meta.csv\")\n",
    "# anomaly_inds = np.argwhere(train_meta[\"image_id\"].isin(anomaly_image_ids).values)[:, 0]\n",
    "\n",
    "# vinbigdata_metadata = MetadataCatalog.get(\"vinbigdata_train\")\n",
    "\n",
    "# cols = 3\n",
    "# rows = 3\n",
    "# fig, axes = plt.subplots(rows, cols, figsize=(18, 18))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for index, anom_ind in enumerate(anomaly_inds[:cols * rows]):\n",
    "#     ax = axes[index]\n",
    "#     # print(anom_ind)\n",
    "#     d = dataset_dicts[anom_ind]\n",
    "#     img = cv2.imread(d[\"file_name\"])\n",
    "#     visualizer = Visualizer(img[:, :, ::-1], metadata=vinbigdata_metadata, scale=0.5)\n",
    "#     out = visualizer.draw_dataset_dict(d)\n",
    "#     # cv2_imshow(out.get_image()[:, :, ::-1])\n",
    "#     #cv2.imwrite(str(outdir / f\"vinbigdata{index}.jpg\"), out.get_image()[:, :, ::-1])\n",
    "#     ax.imshow(out.get_image()[:, :, ::-1])\n",
    "#     ax.set_title(f\"{anom_ind}: image_id {anomaly_image_ids[index]}\")\n",
    "#     ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "activated-messaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.OUTPUT_DIR ./output -> /home/yamaguchi-milkcocholate/VinBigData/src/VinBigData-ObjectDetection/results01\n"
     ]
    }
   ],
   "source": [
    "from detectron2.config.config import CfgNode as CN\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.aug_kwargs = CN(config.aug_kwargs)  # pass aug_kwargs to cfg\n",
    "\n",
    "original_output_dir = cfg.OUTPUT_DIR\n",
    "cfg.OUTPUT_DIR = str(base_dir / config.outdir)\n",
    "print(f\"cfg.OUTPUT_DIR {original_output_dir} -> {cfg.OUTPUT_DIR}\")\n",
    "\n",
    "config_name = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "cfg.merge_from_file(model_zoo.get_config_file(config_name))\n",
    "cfg.DATASETS.TRAIN = (\"vinbigdata_train\",)\n",
    "cfg.DATASETS.TEST = (\"vinbigdata_valid\",)\n",
    "cfg.TEST.EVAL_PERIOD = config.eval_period\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = config.num_workers\n",
    "# Let training initialize from model zoo\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_name)\n",
    "cfg.SOLVER.IMS_PER_BATCH = config.batch_size\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = config.lr_scheduler_name\n",
    "cfg.SOLVER.BASE_LR = config.base_lr  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = config.iter\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = config.checkpoint_period  # Small value=Frequent save need a lot of storage.\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = config.roi_batch_size_per_image\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes_nms)\n",
    "# NOTE: this config means the number of classes,\n",
    "# but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "colonial-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(config.device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-exception",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/22 09:13:13 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=15, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=56, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[AlbumentationsMapper] Augmentations used in training: Compose([\n",
      "  HorizontalFlip(always_apply=False, p=0.5),\n",
      "  ShiftScaleRotate(always_apply=False, p=0.5, shift_limit_x=(-0.0625, 0.0625), shift_limit_y=(-0.0625, 0.0625), scale_limit=(-0.15000000000000002, 0.1499999999999999), rotate_limit=(-10, 10), interpolation=1, border_mode=4, value=None, mask_value=None),\n",
      "  RandomBrightnessContrast(always_apply=False, p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
      "], p=1.0, bbox_params={'format': 'pascal_voc', 'label_fields': ['category_ids'], 'min_area': 0.0, 'min_visibility': 0.0, 'check_each_transform': True}, keypoint_params=None, additional_targets={})\n",
      "\u001b[32m[03/22 09:13:13 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 3512 images left.\n",
      "\u001b[32m[03/22 09:13:13 d2.data.build]: \u001b[0mDistribution of instances among all 14 categories:\n",
      "\u001b[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |\n",
      "|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
      "| Aortic enla.. | 2714         |  Atelectasis  | 182          | Calcification | 585          |\n",
      "| Cardiomegaly  | 1920         | Consolidation | 341          |      ILD      | 588          |\n",
      "| Infiltration  | 770          | Lung Opacity  | 1626         |  Nodule/Mass  | 1493         |\n",
      "| Other lesion  | 1486         | Pleural eff.. | 1413         | Pleural thi.. | 3237         |\n",
      "| Pneumothorax  | 106          | Pulmonary f.. | 2663         |               |              |\n",
      "|     total     | 19124        |               |              |               |              |\u001b[0m\n",
      "\u001b[32m[03/22 09:13:13 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[03/22 09:13:13 d2.data.common]: \u001b[0mSerializing 3512 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/22 09:13:13 d2.data.common]: \u001b[0mSerialized dataset takes 3.12 MiB\n",
      "[AlbumentationsMapper] Augmentations used in inference: Compose([\n",
      "], p=1.0, bbox_params={'format': 'pascal_voc', 'label_fields': ['category_ids'], 'min_area': 0.0, 'min_visibility': 0.0, 'check_each_transform': True}, keypoint_params=None, additional_targets={})\n",
      "\u001b[32m[03/22 09:13:13 d2.data.build]: \u001b[0mDistribution of instances among all 14 categories:\n",
      "\u001b[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |\n",
      "|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
      "| Aortic enla.. | 672          |  Atelectasis  | 48           | Calcification | 160          |\n",
      "| Cardiomegaly  | 482          | Consolidation | 94           |      ILD      | 145          |\n",
      "| Infiltration  | 184          | Lung Opacity  | 391          |  Nodule/Mass  | 374          |\n",
      "| Other lesion  | 350          | Pleural eff.. | 358          | Pleural thi.. | 821          |\n",
      "| Pneumothorax  | 25           | Pulmonary f.. | 706          |               |              |\n",
      "|     total     | 4810         |               |              |               |              |\u001b[0m\n",
      "\u001b[32m[03/22 09:13:13 d2.data.common]: \u001b[0mSerializing 882 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/22 09:13:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.78 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (15, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (15,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (56, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (56,) in the model! You might want to double check if this is expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/22 09:13:14 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "[AlbumentationsMapper] Augmentations used in inference: Compose([\n",
      "], p=1.0, bbox_params={'format': 'pascal_voc', 'label_fields': ['category_ids'], 'min_area': 0.0, 'min_visibility': 0.0, 'check_each_transform': True}, keypoint_params=None, additional_targets={})\n",
      "\u001b[32m[03/22 09:13:22 d2.data.common]: \u001b[0mSerializing 882 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/22 09:13:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.78 MiB\n",
      "\u001b[32m[03/22 09:13:22 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'vinbigdata_valid' to COCO format ...)\n",
      "\u001b[32m[03/22 09:13:22 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n",
      "\u001b[32m[03/22 09:13:22 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 882, #annotations: 4810\n",
      "\u001b[32m[03/22 09:13:22 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at '/home/yamaguchi-milkcocholate/VinBigData/src/VinBigData-ObjectDetection/results01/inference/vinbigdata_valid_coco_format.json' ...\n",
      "\u001b[32m[03/22 09:13:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 882 images\n",
      "\u001b[32m[03/22 09:13:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/882. 0.0513 s / img. ETA=0:00:47\n",
      "\u001b[32m[03/22 09:13:29 d2.evaluation.evaluator]: \u001b[0mInference done 104/882. 0.0509 s / img. ETA=0:00:41\n",
      "\u001b[32m[03/22 09:13:34 d2.evaluation.evaluator]: \u001b[0mInference done 198/882. 0.0508 s / img. ETA=0:00:36\n",
      "\u001b[32m[03/22 09:13:39 d2.evaluation.evaluator]: \u001b[0mInference done 285/882. 0.0513 s / img. ETA=0:00:32\n",
      "\u001b[32m[03/22 09:13:44 d2.evaluation.evaluator]: \u001b[0mInference done 373/882. 0.0519 s / img. ETA=0:00:28\n",
      "\u001b[32m[03/22 09:13:49 d2.evaluation.evaluator]: \u001b[0mInference done 461/882. 0.0522 s / img. ETA=0:00:23\n",
      "\u001b[32m[03/22 09:13:54 d2.evaluation.evaluator]: \u001b[0mInference done 549/882. 0.0524 s / img. ETA=0:00:18\n",
      "\u001b[32m[03/22 09:13:59 d2.evaluation.evaluator]: \u001b[0mInference done 637/882. 0.0526 s / img. ETA=0:00:13\n",
      "\u001b[32m[03/22 09:14:04 d2.evaluation.evaluator]: \u001b[0mInference done 725/882. 0.0527 s / img. ETA=0:00:08\n",
      "\u001b[32m[03/22 09:14:09 d2.evaluation.evaluator]: \u001b[0mInference done 810/882. 0.0528 s / img. ETA=0:00:04\n",
      "\u001b[32m[03/22 09:14:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:49.783243 (0.056765 s / img per device, on 1 devices)\n",
      "\u001b[32m[03/22 09:14:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:46 (0.052845 s / img per device, on 1 devices)\n",
      "Loading and preparing results...\n",
      "DONE (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "COCOeval_opt.evaluate() finished in 0.56 seconds.\n",
      "Accumulating evaluation results...\n",
      "COCOeval_opt.accumulate() finished in 0.20 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.40:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.40:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.40:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.40:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area=   all | maxDets= 10 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area=   all | maxDets=100 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area=medium | maxDets=100 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area= large | maxDets=100 ] = 0.006\n",
      "\u001b[32m[03/22 09:14:15 d2.engine.defaults]: \u001b[0mEvaluation results for vinbigdata_valid in csv format:\n",
      "\u001b[32m[03/22 09:14:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[03/22 09:14:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[03/22 09:14:15 d2.evaluation.testing]: \u001b[0mcopypaste: 0.0003,0.0002,0.0019,0.0000,0.0013,0.0004\n",
      "\u001b[32m[03/22 09:14:16 detectron2]: \u001b[0mLoss on Validation  done 11/882. 0.0000 s / img. ETA=0:00:41\n",
      "\u001b[32m[03/22 09:14:21 detectron2]: \u001b[0mLoss on Validation  done 100/882. 0.0000 s / img. ETA=0:00:43\n",
      "\u001b[32m[03/22 09:14:26 detectron2]: \u001b[0mLoss on Validation  done 185/882. 0.0000 s / img. ETA=0:00:40\n",
      "\u001b[32m[03/22 09:14:31 detectron2]: \u001b[0mLoss on Validation  done 269/882. 0.0000 s / img. ETA=0:00:35\n",
      "\u001b[32m[03/22 09:14:36 detectron2]: \u001b[0mLoss on Validation  done 354/882. 0.0000 s / img. ETA=0:00:30\n",
      "\u001b[32m[03/22 09:14:41 detectron2]: \u001b[0mLoss on Validation  done 439/882. 0.0000 s / img. ETA=0:00:25\n",
      "\u001b[32m[03/22 09:14:46 detectron2]: \u001b[0mLoss on Validation  done 524/882. 0.0000 s / img. ETA=0:00:21\n",
      "\u001b[32m[03/22 09:14:51 detectron2]: \u001b[0mLoss on Validation  done 608/882. 0.0000 s / img. ETA=0:00:16\n",
      "\u001b[32m[03/22 09:14:56 detectron2]: \u001b[0mLoss on Validation  done 693/882. 0.0000 s / img. ETA=0:00:11\n",
      "\u001b[32m[03/22 09:15:01 detectron2]: \u001b[0mLoss on Validation  done 778/882. 0.0000 s / img. ETA=0:00:06\n",
      "\u001b[32m[03/22 09:15:06 detectron2]: \u001b[0mLoss on Validation  done 863/882. 0.0000 s / img. ETA=0:00:01\n",
      "validation do loss eval 3.2273456055181606\n",
      "\u001b[32m[03/22 09:15:08 d2.utils.events]: \u001b[0m eta: 0:06:15  iter: 19  total_loss: 3.128  loss_cls: 2.547  loss_box_reg: 0.06559  loss_rpn_cls: 0.4801  loss_rpn_loc: 0.05431  validation_loss: 3.227  time: 0.3828  data_time: 0.0352  lr: 4.9953e-06  max_mem: 4847M\n",
      "[AlbumentationsMapper] Augmentations used in inference: Compose([\n",
      "], p=1.0, bbox_params={'format': 'pascal_voc', 'label_fields': ['category_ids'], 'min_area': 0.0, 'min_visibility': 0.0, 'check_each_transform': True}, keypoint_params=None, additional_targets={})\n",
      "\u001b[32m[03/22 09:15:15 d2.data.common]: \u001b[0mSerializing 882 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/22 09:15:15 d2.data.common]: \u001b[0mSerialized dataset takes 0.78 MiB\n",
      "\u001b[32m[03/22 09:15:15 d2.evaluation.evaluator]: \u001b[0mStart inference on 882 images\n",
      "\u001b[32m[03/22 09:15:16 d2.evaluation.evaluator]: \u001b[0mInference done 11/882. 0.0515 s / img. ETA=0:00:47\n",
      "\u001b[32m[03/22 09:15:21 d2.evaluation.evaluator]: \u001b[0mInference done 106/882. 0.0504 s / img. ETA=0:00:41\n",
      "\u001b[32m[03/22 09:15:26 d2.evaluation.evaluator]: \u001b[0mInference done 196/882. 0.0514 s / img. ETA=0:00:37\n",
      "\u001b[32m[03/22 09:15:31 d2.evaluation.evaluator]: \u001b[0mInference done 285/882. 0.0519 s / img. ETA=0:00:32\n",
      "\u001b[32m[03/22 09:15:37 d2.evaluation.evaluator]: \u001b[0mInference done 374/882. 0.0522 s / img. ETA=0:00:28\n",
      "\u001b[32m[03/22 09:15:42 d2.evaluation.evaluator]: \u001b[0mInference done 463/882. 0.0524 s / img. ETA=0:00:23\n",
      "\u001b[32m[03/22 09:15:47 d2.evaluation.evaluator]: \u001b[0mInference done 554/882. 0.0523 s / img. ETA=0:00:18\n",
      "\u001b[32m[03/22 09:15:52 d2.evaluation.evaluator]: \u001b[0mInference done 648/882. 0.0520 s / img. ETA=0:00:12\n",
      "\u001b[32m[03/22 09:15:57 d2.evaluation.evaluator]: \u001b[0mInference done 738/882. 0.0519 s / img. ETA=0:00:07\n",
      "\u001b[32m[03/22 09:16:02 d2.evaluation.evaluator]: \u001b[0mInference done 827/882. 0.0520 s / img. ETA=0:00:03\n",
      "\u001b[32m[03/22 09:16:05 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:48.744354 (0.055581 s / img per device, on 1 devices)\n",
      "\u001b[32m[03/22 09:16:05 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:45 (0.052092 s / img per device, on 1 devices)\n",
      "Loading and preparing results...\n",
      "DONE (t=0.27s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "COCOeval_opt.evaluate() finished in 0.52 seconds.\n",
      "Accumulating evaluation results...\n",
      "COCOeval_opt.accumulate() finished in 0.19 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.40:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.40:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.40:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.40:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area=   all | maxDets= 10 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area=   all | maxDets=100 ] = 0.004\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area=medium | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.40:0.95 | area= large | maxDets=100 ] = 0.007\n",
      "\u001b[32m[03/22 09:16:07 d2.engine.defaults]: \u001b[0mEvaluation results for vinbigdata_valid in csv format:\n",
      "\u001b[32m[03/22 09:16:07 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[03/22 09:16:07 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[03/22 09:16:07 d2.evaluation.testing]: \u001b[0mcopypaste: 0.0006,0.0004,0.0051,0.0000,0.0027,0.0008\n",
      "\u001b[32m[03/22 09:16:08 detectron2]: \u001b[0mLoss on Validation  done 11/882. 0.0000 s / img. ETA=0:00:43\n",
      "\u001b[32m[03/22 09:16:13 detectron2]: \u001b[0mLoss on Validation  done 97/882. 0.0000 s / img. ETA=0:00:45\n",
      "\u001b[32m[03/22 09:16:18 detectron2]: \u001b[0mLoss on Validation  done 183/882. 0.0000 s / img. ETA=0:00:40\n",
      "\u001b[32m[03/22 09:16:23 detectron2]: \u001b[0mLoss on Validation  done 269/882. 0.0000 s / img. ETA=0:00:35\n",
      "\u001b[32m[03/22 09:16:28 detectron2]: \u001b[0mLoss on Validation  done 354/882. 0.0000 s / img. ETA=0:00:30\n",
      "\u001b[32m[03/22 09:16:33 detectron2]: \u001b[0mLoss on Validation  done 439/882. 0.0000 s / img. ETA=0:00:26\n",
      "\u001b[32m[03/22 09:16:38 detectron2]: \u001b[0mLoss on Validation  done 524/882. 0.0000 s / img. ETA=0:00:21\n",
      "\u001b[32m[03/22 09:16:43 detectron2]: \u001b[0mLoss on Validation  done 609/882. 0.0000 s / img. ETA=0:00:16\n"
     ]
    }
   ],
   "source": [
    "trainer = MyTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_json(base_dir / config.outdir / \"metrics.json\", orient=\"records\", lines=True)\n",
    "mdf = metrics_df.sort_values(\"iteration\")\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "important-runner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Loss curve\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "mdf1 = mdf[~mdf[\"total_loss\"].isna()]\n",
    "ax.plot(mdf1[\"iteration\"], mdf1[\"total_loss\"], c=\"C0\", label=\"train\")\n",
    "if \"validation_loss\" in mdf.columns:\n",
    "    mdf2 = mdf[~mdf[\"validation_loss\"].isna()]\n",
    "    ax.plot(mdf2[\"iteration\"], mdf2[\"validation_loss\"], c=\"C1\", label=\"validation\")\n",
    "\n",
    "# ax.set_ylim([0, 0.5])\n",
    "ax.legend()\n",
    "ax.set_title(\"Loss curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hourly-photograph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "mdf3 = mdf[~mdf[\"bbox/AP75\"].isna()]\n",
    "ax.plot(mdf3[\"iteration\"], mdf3[\"bbox/AP75\"] / 100., c=\"C2\", label=\"validation\")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(\"AP40\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "material-morning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "mdf_bbox_class = mdf3.iloc[-1][[f\"bbox/AP-{col}\" for col in classes_nms]]\n",
    "mdf_bbox_class.plot(kind=\"bar\", ax=ax)\n",
    "_ = ax.set_title(\"AP by class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-victoria",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "warming-bruce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.OUTPUT_DIR ./output -> /home/yamaguchi-milkcocholate/VinBigData/src/VinBigData-ObjectDetection/results01\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "original_output_dir = cfg.OUTPUT_DIR\n",
    "cfg.OUTPUT_DIR = str(base_dir / config.outdir)\n",
    "print(f\"cfg.OUTPUT_DIR {original_output_dir} -> {cfg.OUTPUT_DIR}\")\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"vinbigdata_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "# cfg.DATASETS.TEST = (\"vinbigdata_train\",)\n",
    "# cfg.TEST.EVAL_PERIOD = 50\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "# Let training initialize from model zoo\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.SOLVER.IMS_PER_BATCH = config.batch_size\n",
    "cfg.SOLVER.BASE_LR = config.base_lr  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = config.iter\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = config.roi_batch_size_per_image\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes_nms)\n",
    "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "hairy-honduras",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from cache dataset_dicts_cache_test_png1024_debug0.pkl\n"
     ]
    }
   ],
   "source": [
    "test_meta = pd.read_csv(str(base_dir / config.test_meta_file))\n",
    "dataset_dicts_test = get_vinbigdata_dicts_test(\n",
    "    base_dir / config.imgdir_name, \n",
    "    test_meta,\n",
    "    test_data_type=f'png{config.img_size}',\n",
    "    debug=config.debug\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "strange-cookbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original thresh 0.0\n",
      "Changed  thresh 0.0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Dataset 'vinbigdata_test' is already registered!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-6fbea3f3d1fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m DatasetCatalog.register(\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"vinbigdata_test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     lambda: get_vinbigdata_dicts_test(\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mbase_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgdir_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtest_meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/VinBigData/lib/python3.7/site-packages/detectron2/data/catalog.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, name, func)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \"\"\"\n\u001b[1;32m     36\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You must register a function with `DatasetCatalog.register`!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dataset '{}' is already registered!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Dataset 'vinbigdata_test' is already registered!"
     ]
    }
   ],
   "source": [
    "### --- Inference & Evaluation ---\n",
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "# path to the model we just trained\n",
    "cfg.MODEL.WEIGHTS = str(base_dir / config.outdir /\"model_final.pth\")\n",
    "print(\"Original thresh\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)  # 0.05\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0  # set a custom testing threshold\n",
    "print(\"Changed  thresh\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "DatasetCatalog.register(\n",
    "    \"vinbigdata_test\", \n",
    "    lambda: get_vinbigdata_dicts_test(\n",
    "        base_dir / config.imgdir_name,\n",
    "        test_meta,\n",
    "        debug=debug\n",
    "    )\n",
    ")\n",
    "MetadataCatalog.get(\"vinbigdata_test\").set(thing_classes=classes_nms)\n",
    "metadata = MetadataCatalog.get(\"vinbigdata_test\")\n",
    "\n",
    "if config.debug:\n",
    "    dataset_dicts_test = dataset_dicts_test[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "digital-procedure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1839128181514d4fabdd96bca7aa8377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_list = []\n",
    "index = 0\n",
    "batch_size = 4\n",
    "\n",
    "for i in tqdm(range(ceil(len(dataset_dicts_test) / batch_size))):\n",
    "    inds = list(range(batch_size * i, min(batch_size * (i + 1), len(dataset_dicts_test))))\n",
    "    dataset_dicts_test_batch = [dataset_dicts_test[i] for i in inds]\n",
    "    im_list = [cv2.imread(d[\"file_name\"]) for d in dataset_dicts_test_batch]\n",
    "    outputs_list = predict_batch(predictor, im_list)\n",
    "\n",
    "    for im, outputs, d in zip(im_list, outputs_list, dataset_dicts_test_batch):\n",
    "        resized_height, resized_width, ch = im.shape\n",
    "        # outputs = predictor(im)\n",
    "        if index < 5:\n",
    "            # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "            v = Visualizer(\n",
    "                im[:, :, ::-1],\n",
    "                metadata=metadata,\n",
    "                scale=0.5,\n",
    "                instance_mode=ColorMode.IMAGE_BW\n",
    "                # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "            )\n",
    "            out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "            # cv2_imshow(out.get_image()[:, :, ::-1])\n",
    "            cv2.imwrite(str(base_dir / config.outdir / f\"pred_{index}.jpg\"), out.get_image()[:, :, ::-1])\n",
    "\n",
    "        image_id, dim0, dim1 = test_meta.iloc[index].values\n",
    "\n",
    "        instances = outputs[\"instances\"]\n",
    "        if len(instances) == 0:\n",
    "            # No finding, let's set 14 1 0 0 1 1x.\n",
    "            result = {\"image_id\": image_id, \"PredictionString\": \"14 1.0 0 0 1 1\"}\n",
    "        else:\n",
    "            # Find some bbox...\n",
    "            # print(f\"index={index}, find {len(instances)} bbox.\")\n",
    "            fields: Dict[str, Any] = instances.get_fields()\n",
    "            pred_classes = fields[\"pred_classes\"]  # (n_boxes,)\n",
    "            pred_scores = fields[\"scores\"]\n",
    "            # shape (n_boxes, 4). (xmin, ymin, xmax, ymax)\n",
    "            pred_boxes = fields[\"pred_boxes\"].tensor\n",
    "\n",
    "            h_ratio = dim0 / resized_height\n",
    "            w_ratio = dim1 / resized_width\n",
    "            pred_boxes[:, [0, 2]] *= w_ratio\n",
    "            pred_boxes[:, [1, 3]] *= h_ratio\n",
    "\n",
    "            pred_classes_array = pred_classes.cpu().numpy()\n",
    "            pred_boxes_array = pred_boxes.cpu().numpy()\n",
    "            pred_scores_array = pred_scores.cpu().numpy()\n",
    "\n",
    "            result = {\n",
    "                \"image_id\": image_id,\n",
    "                \"PredictionString\": format_pred(\n",
    "                    pred_classes_array, pred_boxes_array, pred_scores_array\n",
    "                ),\n",
    "            }\n",
    "        results_list.append(result)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "diverse-terminal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8dec5497ecc246766acfba5a4be4e619</td>\n",
       "      <td>0 0.9293931722640991 1026 606 1248 877 11 0.49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287422bed1d9d153387361889619abed</td>\n",
       "      <td>3 0.9719218611717224 664 1195 1834 1724 0 0.88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1d12b94b7acbeadef7d7700b50aa90d4</td>\n",
       "      <td>0 0.9262945055961609 1181 895 1434 1156 3 0.88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6b872791e23742f6c33a08fc24f77365</td>\n",
       "      <td>0 0.6138948798179626 1048 890 1254 1136 11 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d0d2addff91ad7beb1d92126ff74d621</td>\n",
       "      <td>0 0.9376397132873535 1422 827 1702 1143 3 0.81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>78b44b96b121d6075d7ae27135278e03</td>\n",
       "      <td>0 0.7779184579849243 1044 771 1211 961 3 0.576...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>afee8ff90f29b8827d0eb78774d25324</td>\n",
       "      <td>0 0.7558690309524536 1044 714 1246 964 3 0.280...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>6e07fab2014be723250f7897ab6e3df2</td>\n",
       "      <td>0 0.9621807932853699 1641 784 1972 1113 3 0.94...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>690bb572300ef08bbbb7ebf4196099cf</td>\n",
       "      <td>0 0.8978896737098694 1089 696 1340 945 3 0.207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0a08191a658edb1327e7282045ec71cf</td>\n",
       "      <td>0 0.5477412939071655 1116 783 1288 993 11 0.16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image_id  \\\n",
       "0     8dec5497ecc246766acfba5a4be4e619   \n",
       "1     287422bed1d9d153387361889619abed   \n",
       "2     1d12b94b7acbeadef7d7700b50aa90d4   \n",
       "3     6b872791e23742f6c33a08fc24f77365   \n",
       "4     d0d2addff91ad7beb1d92126ff74d621   \n",
       "...                                ...   \n",
       "2995  78b44b96b121d6075d7ae27135278e03   \n",
       "2996  afee8ff90f29b8827d0eb78774d25324   \n",
       "2997  6e07fab2014be723250f7897ab6e3df2   \n",
       "2998  690bb572300ef08bbbb7ebf4196099cf   \n",
       "2999  0a08191a658edb1327e7282045ec71cf   \n",
       "\n",
       "                                       PredictionString  \n",
       "0     0 0.9293931722640991 1026 606 1248 877 11 0.49...  \n",
       "1     3 0.9719218611717224 664 1195 1834 1724 0 0.88...  \n",
       "2     0 0.9262945055961609 1181 895 1434 1156 3 0.88...  \n",
       "3     0 0.6138948798179626 1048 890 1254 1136 11 0.3...  \n",
       "4     0 0.9376397132873535 1422 827 1702 1143 3 0.81...  \n",
       "...                                                 ...  \n",
       "2995  0 0.7779184579849243 1044 771 1211 961 3 0.576...  \n",
       "2996  0 0.7558690309524536 1044 714 1246 964 3 0.280...  \n",
       "2997  0 0.9621807932853699 1641 784 1972 1113 3 0.94...  \n",
       "2998  0 0.8978896737098694 1089 696 1340 945 3 0.207...  \n",
       "2999  0 0.5477412939071655 1116 783 1288 993 11 0.16...  \n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This submission includes only detection model's predictions\n",
    "submission_det = pd.DataFrame(results_list, columns=['image_id', 'PredictionString'])\n",
    "submission_det.to_csv(base_dir / config.outdir/\"submission.csv\", index=False)\n",
    "submission_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "alone-disabled",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class0</th>\n",
       "      <th>class1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8dec5497ecc246766acfba5a4be4e619</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287422bed1d9d153387361889619abed</td>\n",
       "      <td>0.049089</td>\n",
       "      <td>0.950911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1d12b94b7acbeadef7d7700b50aa90d4</td>\n",
       "      <td>0.991706</td>\n",
       "      <td>0.008294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6b872791e23742f6c33a08fc24f77365</td>\n",
       "      <td>0.788935</td>\n",
       "      <td>0.211065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d0d2addff91ad7beb1d92126ff74d621</td>\n",
       "      <td>0.995391</td>\n",
       "      <td>0.004608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>78b44b96b121d6075d7ae27135278e03</td>\n",
       "      <td>0.999924</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>afee8ff90f29b8827d0eb78774d25324</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>6e07fab2014be723250f7897ab6e3df2</td>\n",
       "      <td>0.996261</td>\n",
       "      <td>0.003739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>690bb572300ef08bbbb7ebf4196099cf</td>\n",
       "      <td>0.939969</td>\n",
       "      <td>0.060031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0a08191a658edb1327e7282045ec71cf</td>\n",
       "      <td>0.990715</td>\n",
       "      <td>0.009285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image_id    class0    class1\n",
       "0     8dec5497ecc246766acfba5a4be4e619  0.999985  0.000015\n",
       "1     287422bed1d9d153387361889619abed  0.049089  0.950911\n",
       "2     1d12b94b7acbeadef7d7700b50aa90d4  0.991706  0.008294\n",
       "3     6b872791e23742f6c33a08fc24f77365  0.788935  0.211065\n",
       "4     d0d2addff91ad7beb1d92126ff74d621  0.995391  0.004608\n",
       "...                                ...       ...       ...\n",
       "2995  78b44b96b121d6075d7ae27135278e03  0.999924  0.000076\n",
       "2996  afee8ff90f29b8827d0eb78774d25324  0.999992  0.000008\n",
       "2997  6e07fab2014be723250f7897ab6e3df2  0.996261  0.003739\n",
       "2998  690bb572300ef08bbbb7ebf4196099cf  0.939969  0.060031\n",
       "2999  0a08191a658edb1327e7282045ec71cf  0.990715  0.009285\n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_2class = pd.read_csv(str(base_dir / 'pfn_copy_test_pred.csv'))\n",
    "pred_test_2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "stock-vegetable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class0</th>\n",
       "      <th>class1</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8dec5497ecc246766acfba5a4be4e619</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0 0.9293931722640991 1026 606 1248 877 11 0.49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287422bed1d9d153387361889619abed</td>\n",
       "      <td>0.049089</td>\n",
       "      <td>0.950911</td>\n",
       "      <td>3 0.9719218611717224 664 1195 1834 1724 0 0.88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1d12b94b7acbeadef7d7700b50aa90d4</td>\n",
       "      <td>0.991706</td>\n",
       "      <td>0.008294</td>\n",
       "      <td>0 0.9262945055961609 1181 895 1434 1156 3 0.88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6b872791e23742f6c33a08fc24f77365</td>\n",
       "      <td>0.788935</td>\n",
       "      <td>0.211065</td>\n",
       "      <td>0 0.6138948798179626 1048 890 1254 1136 11 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d0d2addff91ad7beb1d92126ff74d621</td>\n",
       "      <td>0.995391</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>0 0.9376397132873535 1422 827 1702 1143 3 0.81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>78b44b96b121d6075d7ae27135278e03</td>\n",
       "      <td>0.999924</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0 0.7779184579849243 1044 771 1211 961 3 0.576...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>afee8ff90f29b8827d0eb78774d25324</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0 0.7558690309524536 1044 714 1246 964 3 0.280...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>6e07fab2014be723250f7897ab6e3df2</td>\n",
       "      <td>0.996261</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>0 0.9621807932853699 1641 784 1972 1113 3 0.94...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>690bb572300ef08bbbb7ebf4196099cf</td>\n",
       "      <td>0.939969</td>\n",
       "      <td>0.060031</td>\n",
       "      <td>0 0.8978896737098694 1089 696 1340 945 3 0.207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0a08191a658edb1327e7282045ec71cf</td>\n",
       "      <td>0.990715</td>\n",
       "      <td>0.009285</td>\n",
       "      <td>0 0.5477412939071655 1116 783 1288 993 11 0.16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image_id    class0    class1  \\\n",
       "0     8dec5497ecc246766acfba5a4be4e619  0.999985  0.000015   \n",
       "1     287422bed1d9d153387361889619abed  0.049089  0.950911   \n",
       "2     1d12b94b7acbeadef7d7700b50aa90d4  0.991706  0.008294   \n",
       "3     6b872791e23742f6c33a08fc24f77365  0.788935  0.211065   \n",
       "4     d0d2addff91ad7beb1d92126ff74d621  0.995391  0.004608   \n",
       "...                                ...       ...       ...   \n",
       "2995  78b44b96b121d6075d7ae27135278e03  0.999924  0.000076   \n",
       "2996  afee8ff90f29b8827d0eb78774d25324  0.999992  0.000008   \n",
       "2997  6e07fab2014be723250f7897ab6e3df2  0.996261  0.003739   \n",
       "2998  690bb572300ef08bbbb7ebf4196099cf  0.939969  0.060031   \n",
       "2999  0a08191a658edb1327e7282045ec71cf  0.990715  0.009285   \n",
       "\n",
       "                                       PredictionString  \n",
       "0     0 0.9293931722640991 1026 606 1248 877 11 0.49...  \n",
       "1     3 0.9719218611717224 664 1195 1834 1724 0 0.88...  \n",
       "2     0 0.9262945055961609 1181 895 1434 1156 3 0.88...  \n",
       "3     0 0.6138948798179626 1048 890 1254 1136 11 0.3...  \n",
       "4     0 0.9376397132873535 1422 827 1702 1143 3 0.81...  \n",
       "...                                                 ...  \n",
       "2995  0 0.7779184579849243 1044 771 1211 961 3 0.576...  \n",
       "2996  0 0.7558690309524536 1044 714 1246 964 3 0.280...  \n",
       "2997  0 0.9621807932853699 1641 784 1972 1113 3 0.94...  \n",
       "2998  0 0.8978896737098694 1089 696 1340 945 3 0.207...  \n",
       "2999  0 0.5477412939071655 1116 783 1288 993 11 0.16...  \n",
       "\n",
       "[3000 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.merge(pred_test_2class, submission_det, how='left', on='image_id')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "tired-opposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08eabe7344df4fa79c25678b45614b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "low_threshold = 0\n",
    "high_threshold = 0.976\n",
    "\n",
    "for i in tqdm(submission.index):\n",
    "    p0 = submission.loc[i, 'class0']\n",
    "    \n",
    "    if p0 < low_threshold:\n",
    "        pass\n",
    "    elif low_threshold <= p0 and p0 < high_threshold:\n",
    "        submission.loc[i, 'PredictionString'] += f\" 14 {p0} 0 0 1 1\"\n",
    "    else:\n",
    "        submission.loc[i, 'PredictionString'] = '14 1 0 0 1 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "plastic-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.drop(columns=['class0', 'class1'])\n",
    "submission.to_csv(str(base_dir / 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "inclusive-recorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8dec5497ecc246766acfba5a4be4e619</td>\n",
       "      <td>14 1 0 0 1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287422bed1d9d153387361889619abed</td>\n",
       "      <td>3 0.9719218611717224 664 1195 1834 1724 0 0.88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1d12b94b7acbeadef7d7700b50aa90d4</td>\n",
       "      <td>14 1 0 0 1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6b872791e23742f6c33a08fc24f77365</td>\n",
       "      <td>0 0.6138948798179626 1048 890 1254 1136 11 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d0d2addff91ad7beb1d92126ff74d621</td>\n",
       "      <td>14 1 0 0 1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>78b44b96b121d6075d7ae27135278e03</td>\n",
       "      <td>14 1 0 0 1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>afee8ff90f29b8827d0eb78774d25324</td>\n",
       "      <td>14 1 0 0 1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>6e07fab2014be723250f7897ab6e3df2</td>\n",
       "      <td>14 1 0 0 1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>690bb572300ef08bbbb7ebf4196099cf</td>\n",
       "      <td>0 0.8978896737098694 1089 696 1340 945 3 0.207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0a08191a658edb1327e7282045ec71cf</td>\n",
       "      <td>14 1 0 0 1 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image_id  \\\n",
       "0     8dec5497ecc246766acfba5a4be4e619   \n",
       "1     287422bed1d9d153387361889619abed   \n",
       "2     1d12b94b7acbeadef7d7700b50aa90d4   \n",
       "3     6b872791e23742f6c33a08fc24f77365   \n",
       "4     d0d2addff91ad7beb1d92126ff74d621   \n",
       "...                                ...   \n",
       "2995  78b44b96b121d6075d7ae27135278e03   \n",
       "2996  afee8ff90f29b8827d0eb78774d25324   \n",
       "2997  6e07fab2014be723250f7897ab6e3df2   \n",
       "2998  690bb572300ef08bbbb7ebf4196099cf   \n",
       "2999  0a08191a658edb1327e7282045ec71cf   \n",
       "\n",
       "                                       PredictionString  \n",
       "0                                          14 1 0 0 1 1  \n",
       "1     3 0.9719218611717224 664 1195 1834 1724 0 0.88...  \n",
       "2                                          14 1 0 0 1 1  \n",
       "3     0 0.6138948798179626 1048 890 1254 1136 11 0.3...  \n",
       "4                                          14 1 0 0 1 1  \n",
       "...                                                 ...  \n",
       "2995                                       14 1 0 0 1 1  \n",
       "2996                                       14 1 0 0 1 1  \n",
       "2997                                       14 1 0 0 1 1  \n",
       "2998  0 0.8978896737098694 1089 696 1340 945 3 0.207...  \n",
       "2999                                       14 1 0 0 1 1  \n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-question",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
